# План разработки расширения MLOps системы с автоматическим масштабированием и мониторингом

## Прогресс выполнения

- **Итерация 1:** ✅ Завершена
- **Итерация 2:** ✅ Завершена  
- **Итерация 3:** ✅ Завершена
- **Итерация 4:** ✅ Завершена
- **Итерация 5:** ⬜ Не начата

---

## Итерация 1: Копирование базовых компонентов из Tasks9
- [x] Копирование структуры проекта из Tasks9
- [x] Копирование папки `src/` с ML API (api.py, config.py, model_loader.py)
- [x] Копирование папки `argocd/` с Kubernetes манифестами
- [x] Копирование Dockerfile и requirements.txt
- [x] Копирование .env-example и создание .env для Tasks10
- [x] Адаптация argocd/application.yaml под Tasks10 (изменение имени приложения)
- [x] Тестирование работы скопированного API локально

**Критерии тестирования:**
- Скопированный API успешно запускается локально
- Существующая модель из Tasks9 корректно работает (POST /predict возвращает результат)
- Эндпоинты /predict и /health работают корректно (статус 200)
- Docker образ собирается без ошибок (`docker build` успешен)
- Kubernetes манифесты проходят валидацию (`kubectl apply --dry-run=client`)

---

## Итерация 2: Добавление метрик Prometheus
- [x] Создание модуля src/metrics.py с определениями метрик
- [x] Расширение src/api.py эндпоинтом /metrics
- [x] Добавление метрик в эндпоинт /predict (predictions_total, prediction_duration)
- [x] Добавление HTTP метрик (requests_total, request_duration, errors_total)
- [x] Добавление метрик ошибок и производительности
- [x] Обновление requirements.txt (добавление prometheus_client==0.19.0)
- [x] Тестирование сбора метрик

**Критерии тестирования (локально):**
- Docker образ собирается с метриками: `docker build -t tasks10-ml-api:metrics .`
- Docker контейнер запускается: `docker run -p 8000:8000 tasks10-ml-api:metrics`
- Эндпоинт метрик работает: `curl http://localhost:8000/metrics` возвращает метрики в формате Prometheus
- После запроса к /predict метрики `ml_model_predictions_total` увеличиваются
- HTTP метрики `http_requests_total` корректно отслеживают запросы по статус-кодам
- Метрики ошибок `ml_model_errors_total` инкрементируются при ошибках
- `requirements.txt` содержит `prometheus_client==0.19.0`

---

## Итерация 3: Настройка автоматического масштабирования и развертывание
- [x] Корректировка argocd/manifests/hpa.yaml для 4-6 реплик
- [x] Настройка триггеров HPA на CPU > 80% и Memory > 70%
- [x] Создание argocd/manifests/servicemonitor.yaml для интеграции с Prometheus
- [x] Обновление argocd/README.md под Tasks10
- [x] Развертывание через ArgoCD в готовый кластер
- [x] Применение HPA к существующему Deployment
- [x] Тестирование автоматического масштабирования

**Критерии тестирования:**
- ArgoCD развертывание успешно:
  - `kubectl apply -f argocd/project.yaml` создает ArgoCD Project
  - `kubectl apply -f argocd/application.yaml` создает ArgoCD Application
  - `argocd app sync tasks10-ml-service` успешно синхронизирует
  - `kubectl get pods,svc,deployment -n mlops-tasks10` показывает готовые ресурсы
  - Поды переходят в статус Running
- `kubectl get hpa -n mlops-tasks10` показывает HPA с min=4, max=6 реплик
- Метрики в Kubernetes работают:
  - `kubectl port-forward svc/tasks10-ml-service 8000:80 -n mlops-tasks10`
  - `curl http://localhost:8000/metrics` возвращает метрики
  - `curl http://tasks10-ml-api.darveter.com/metrics` через Ingress
- В Prometheus UI (через port-forward) метрики `ml_model_*` доступны и обновляются
- ServiceMonitor создает target в Prometheus с статусом UP
- При нагрузке `kubectl get pods -n mlops-tasks10` показывает увеличение реплик
- HPA не превышает лимит в 6 реплик при высокой нагрузке

---

## Итерация 4: Создание системы мониторинга
- [x] Создание monitoring/dashboards/ml-model-dashboard.json для Grafana
- [x] Создание monitoring/alerts/ml-alerts.yaml с правилами AlertManager
- [x] Настройка алертов: CPU>80%, ErrorRate>5%, ResponseTime>2s, KafkaQueue>1000
- [x] Настройка специального алерта: уведомление администратора при 6 экземплярах и CPU>80% в течение 5 минут
- [x] Импорт дашборда в существующий Grafana (вручную)
- [ ] Настройка правил алертинга в существующем AlertManager (вручную)


**Критерии тестирования:**
- Grafana дашборд отображает метрики ML модели
- Алерты срабатывают при превышении пороговых значений
- AlertManager отправляет уведомления при критических событиях
- Дашборд показывает производительность, ошибки и нагрузку
- Система мониторинга работает в реальном времени

---

## Итерация 5: Тестирование имитации атаки через Apache Kafka

**Цель**: Протестировать имитацию атаки подняв сервис Apache Kafka из предыдущего задания и наращивания интенсивности потока событий до срабатывания алерта администратора (6 экземпляров + CPU>80% в течение 5 минут).

**1. [x] Скопировать файлы из Tasks8/my-kafka: `producer_replay_transactions.py` → `dag/kafka-attack-producer.py`**
   - **Зачем**: Используем готовый Kafka producer из Tasks8, который умеет генерировать транзакции с разной интенсивностью
   - **Что получим**: Рабочий генератор нагрузки вместо написания с нуля
   - **Детали**: Файл уже настроен на работу с топиком `transactions-input` и знает формат данных ML модели

**2. [x] Модифицировать producer для escalating attack: 50→200→500→1500 TPS вместо фиксированных значений** *(правильный генератор атаки)*
   - **Зачем**: Имитировать реальную атаку с постепенным наращиванием нагрузки до критического уровня
   - **Что получим**: Сценарий атаки который гарантированно приведет к масштабированию до 6 подов
   - **Детали**: 50 TPS (норма) → 200 TPS (начало HPA) → 500 TPS (4-5 подов) → 1500 TPS (6 подов + CPU>80%)

**3. [x] Корректировка AirFlow в кластере для подключения внешнего git репозитория с DAGами**
   - **Зачем**: Настроить автоматическую синхронизацию DAG-ов из внешнего git репозитория для запуска Kafka producer и Spark streaming
   - **Что получим**: AirFlow будет автоматически подхватывать DAG-и из Tasks8 без ручного копирования
   - **Подход**: Использовать готовую конфигурацию из `airflow/airflow-helm.yaml` и настроить Git Sync:
     - Настроить `dags.gitSync` в Helm values для подключения к git репозиторию
     - Использовать DAG-и из Tasks8: `producer_replay_transactions.py`, `spark_stream_inference_local.py`
     - Настроить автоматическое обновление DAG-ов каждые 60 секунд
   - **Файлы**: Модификация `airflow/airflow-helm.yaml` с добавлением git sync конфигурации


**4. [ ] Модифицировать Spark streaming для вызова ML API вместо локального inference** *(правильный маршрут нагрузки)*
   - **Зачем**: Создать реальную нагрузку на ML API через Kafka, используя готовую инфраструктуру Spark
   - **Что получим**: Каждое сообщение из Kafka будет генерировать HTTP запрос к ML API
   - **Подход**: Создать новый `spark_stream_inference_http.py` в `dag/` на основе Tasks8:
     - ✅ **СОЗДАНО**: `/Tasks10/dag/spark_stream_inference_http.py`
     - Заменить локальный MLflow inference → на HTTP POST к нашему ML API
     - Spark читает из `transactions-input` → HTTP запрос к `http://tasks10-ml-api.darveter.com/predict` → CPU нагрузка
     - Результат: **Kafka Producer** → **Spark Streaming** → **HTTP POST** → **ML API** → **HPA масштабирование**
   - **Преимущества**: Готовая инфраструктура, реальная нагрузка, масштабируемость до тысяч TPS

**5. [ ] Запустить Kafka + Spark streaming: `python spark_stream_inference_local.py` (модифицированный)**
   - **Зачем**: Создать полную инфраструктуру потоковой обработки как в реальной системе
   - **Что получим**: Kafka будет обрабатывать поток транзакций, Spark будет делать HTTP вызовы к ML API
   - **Детали**: Spark streaming читает из `transactions-input` → HTTP POST к ML API → результаты в `fraud-predictions`
   - **Модификация**: Заменить MLflow model.predict() на requests.post() к нашему ML API endpoint

**6. [ ] Запустить escalating attack: `python kafka-attack-producer.py --attack-mode`**
   - **Зачем**: Запустить сценарий атаки для создания критической нагрузки на систему
   - **Что получим**: Постепенное увеличение нагрузки до момента срабатывания алерта администратора
   - **Детали**: Каждая фаза атаки длится несколько минут, чтобы HPA успел среагировать и создать новые поды

**7. [ ] Мониторить масштабирование: `watch kubectl get pods -n mlops-tasks10`**
   - **Зачем**: Отслеживать реакцию Kubernetes HPA на растущую нагрузку в реальном времени
   - **Что получим**: Визуальное подтверждение что система масштабируется: 2→3→4→5→6 подов
   - **Детали**: Следить за статусом подов (Pending→Running) и временем создания новых экземпляров

**8. [ ] Дождаться результата: 1500 TPS → 6 подов + CPU >80% → 🚨 АЛЕРТ АДМИНИСТРАТОРА!**
   - **Зачем**: Подтвердить что алерт `AdminNotification_MaxScaleHighCPU` срабатывает при критических условиях
   - **Что получим**: Проверку всей системы мониторинга: метрики → Prometheus → AlertManager → уведомление админа
   - **Детали**: Алерт должен сработать через 5 минут после достижения условия "6 подов + CPU>80%"

---

## Итерация 6 **Настроить Apache AirFlow**

 Периодическое переобучение модели с фиксацией метрик в MLFlow. Для этого разверните AirFlow в кластере и подключите внешний git репозиторий с DAGами для него.

