#!/usr/bin/env python3
"""
Producer –¥–ª—è replay –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ Kafka
–î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ8 - –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –ø–æ—Ç–æ–∫–µ
–≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
–ü–µ—Ä–µ–¥–µ–ª–∞–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Airflow DAG –∏ Variables
"""

import os
import sys
import json
import time
import logging
from typing import Dict, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass

# S3 –∏ –¥–∞–Ω–Ω—ã–µ
import boto3
import pandas as pd

# PyArrow –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö parquet
try:
    import pyarrow.parquet as pq
    import pyarrow.fs as fs
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –±—É–¥–µ—Ç –≤ –ª–æ–≥–∞—Ö –ø–æ–∑–∂–µ

# Kafka
from kafka import KafkaProducer, errors as kafka_errors

# Airflow Variables (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
try:
    from airflow.models import Variable
    AIRFLOW_AVAILABLE = True
except ImportError:
    AIRFLOW_AVAILABLE = False
    Variable = None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ProducerConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Producer'–∞ –∏–∑ Airflow Variables"""
    # Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    kafka_bootstrap_servers: str = "kafka.kafka.svc.cluster.local:9092"
    kafka_topic: str = "transactions-input"
    kafka_consumer_group: str = "fraud-detection-group"
    
    # Producer –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    tps_target: int = 50
    batch_size: int = 16384
    linger_ms: int = 10
    compression_type: str = "gzip"
    acks: str = "all"
    retries: int = 3
    timeout: int = 1
    max_in_flight: int = 5
    
    # –î–∞–Ω–Ω—ã–µ
    data_path: str = "s3a://otus/clean/fraud_transactions_fixed_new.parquet"
    max_records: Optional[int] = None
    
    # S3 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    s3_endpoint_url: str = "http://192.168.31.201:9000"
    s3_access_key: str = "admin"
    s3_secret_key: str = "password"
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    stats_interval: int = 10
    enable_performance_logging: bool = True


def get_config_from_airflow_variables(tps_override: Optional[int] = None,
                                      topic_override: Optional[str] = None,
                                      max_records_override: Optional[int] = None) -> ProducerConfig:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ Airflow Variables
    
    Args:
        tps_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ TPS
        topic_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ø–∏–∫–∞
        max_records_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
    
    Returns:
        ProducerConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ Airflow Variables
    """
    config = ProducerConfig()
    
    if not AIRFLOW_AVAILABLE or not Variable:
        logger.warning("‚ö†Ô∏è Airflow Variables –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º defaults")
        if tps_override:
            config.tps_target = tps_override
        if topic_override:
            config.kafka_topic = topic_override
        if max_records_override:
            config.max_records = max_records_override
        return config
    
    try:
        # Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.kafka_bootstrap_servers = Variable.get("KAFKA_BOOTSTRAP_SERVERS", default_var=config.kafka_bootstrap_servers)
        config.kafka_topic = Variable.get("KAFKA_INPUT_TOPIC", default_var=config.kafka_topic)
        config.kafka_consumer_group = Variable.get("KAFKA_CONSUMER_GROUP", default_var=config.kafka_consumer_group)
        
        # Producer –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.tps_target = int(Variable.get("PRODUCER_DEFAULT_TPS", default_var=str(config.tps_target)))
        config.batch_size = int(Variable.get("PRODUCER_BATCH_SIZE", default_var=str(config.batch_size)))
        config.linger_ms = int(Variable.get("PRODUCER_LINGER_MS", default_var=str(config.linger_ms)))
        config.compression_type = Variable.get("PRODUCER_COMPRESSION_TYPE", default_var=config.compression_type)
        config.acks = Variable.get("PRODUCER_ACKS", default_var=config.acks)
        config.retries = int(Variable.get("PRODUCER_RETRIES", default_var=str(config.retries)))
        config.timeout = int(Variable.get("PRODUCER_TIMEOUT", default_var=str(config.timeout)))
        config.max_in_flight = int(Variable.get("PRODUCER_MAX_IN_FLIGHT", default_var=str(config.max_in_flight)))
        
        # –î–∞–Ω–Ω—ã–µ
        config.data_path = Variable.get("CLEAN_DATA_PATH", default_var=config.data_path)
        
        # S3 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.s3_endpoint_url = Variable.get("S3_ENDPOINT_URL", default_var=config.s3_endpoint_url)
        config.s3_access_key = Variable.get("S3_ACCESS_KEY", default_var=config.s3_access_key)
        config.s3_secret_key = Variable.get("S3_SECRET_KEY", default_var=config.s3_secret_key)
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        config.stats_interval = int(Variable.get("MONITORING_STATS_INTERVAL", default_var=str(config.stats_interval)))
        config.enable_performance_logging = Variable.get("ENABLE_PERFORMANCE_LOGGING", default_var="true").lower() == "true"
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if tps_override:
            config.tps_target = tps_override
        if topic_override:
            config.kafka_topic = topic_override
        if max_records_override:
            config.max_records = max_records_override
            
        logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Airflow Variables")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ Airflow: {e}")
        logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
        if tps_override:
            config.tps_target = tps_override
        if topic_override:
            config.kafka_topic = topic_override
        if max_records_override:
            config.max_records = max_records_override
    
    return config


@dataclass
class ProducerStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ producer'–∞"""
    start_time: datetime
    messages_sent: int = 0
    messages_error: int = 0
    bytes_sent: int = 0
    last_tps: float = 0.0
    avg_tps: float = 0.0
    kafka_lag: int = 0
    avg_latency: float = 0.0


class TransactionProducer:
    """Producer –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ Kafka —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º TPS"""
    
    def __init__(self, config: ProducerConfig):
        self.config = config
        self.stats = ProducerStats(start_time=datetime.now())
        self.running = False
        
        # Kafka producer
        self.producer = None
        self._init_kafka_producer()
        
        # S3 –∫–ª–∏–µ–Ω—Ç –¥–ª—è MinIO
        self.s3_client = None
        self._init_s3_client()
        
    def _init_kafka_producer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Kafka Producer —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ config"""
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=self.config.kafka_bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: str(k).encode('utf-8'),
                batch_size=self.config.batch_size,
                linger_ms=self.config.linger_ms,
                compression_type=self.config.compression_type,
                max_in_flight_requests_per_connection=self.config.max_in_flight,
                acks=self.config.acks,
                retries=self.config.retries
            )
            logger.info(f"‚úÖ Kafka Producer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.config.kafka_bootstrap_servers}")
            logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: batch_size={self.config.batch_size}, compression={self.config.compression_type}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Kafka Producer: {e}")
            raise
    
    def _init_s3_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è S3 –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MinIO"""
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            os.environ["AWS_ACCESS_KEY_ID"] = self.config.s3_access_key
            os.environ["AWS_SECRET_ACCESS_KEY"] = self.config.s3_secret_key
            
            self.s3_client = boto3.client(
                's3',
                endpoint_url=self.config.s3_endpoint_url,
                aws_access_key_id=self.config.s3_access_key,
                aws_secret_access_key=self.config.s3_secret_key,
                region_name='us-east-1'  # –î–ª—è MinIO
            )
            logger.info(f"‚úÖ S3 –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.config.s3_endpoint_url}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ S3 –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            logger.error("üö® S3 –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã!")
            raise
    
    def _load_data_smart(self, bucket_name: str, key: str) -> pd.DataFrame:
        """
        –£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ —Ñ–∞–π–ª—ã, —Ç–∞–∫ –∏ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ datasets
        
        Args:
            bucket_name: –ò–º—è S3 bucket
            key: –ö–ª—é—á (–ø—É—Ç—å –∫ —Ñ–∞–π–ª—É/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏)
            
        Returns:
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        if not PYARROW_AVAILABLE:
            logger.warning("‚ö†Ô∏è PyArrow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - fallback –Ω–∞ first part-—Ñ–∞–π–ª –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–π")
        else:
            logger.info("‚úÖ PyArrow –¥–æ—Å—Ç—É–ø–µ–Ω - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö datasets")
            
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ S3
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä–µ–∫—Ç–µ –∫–∞–∫ –æ —Ñ–∞–π–ª–µ
                self.s3_client.head_object(Bucket=bucket_name, Key=key)
                is_single_file = True
                logger.info(f"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª: {key}")
                
            except self.s3_client.exceptions.ClientError as e:
                if e.response['Error']['Code'] == '404':
                    # –í–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
                    is_single_file = False
                    logger.info(f"üìÅ –í–æ–∑–º–æ–∂–Ω–æ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {key}")
                else:
                    raise
            
            if is_single_file:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ñ–∞–π–ª
                logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ parquet —Ñ–∞–π–ª–∞...")
                temp_file = "/tmp/fraud_data.parquet"
                self.s3_client.download_file(bucket_name, key, temp_file)
                df = pd.read_parquet(temp_file)
                
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                cleanup_temp = os.getenv("CLEANUP_TEMP_FILES", "true").lower() == "true"
                if cleanup_temp:
                    os.remove(temp_file)
                    logger.info("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
                
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset
                if not PYARROW_AVAILABLE:
                    # Fallback: –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ part-—Ñ–∞–π–ª—ã –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—ã–π
                    logger.warning("‚ö†Ô∏è PyArrow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—â–µ–º part-—Ñ–∞–π–ª—ã...")
                    objects = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=key)
                    
                    if 'Contents' in objects:
                        part_files = [obj['Key'] for obj in objects['Contents'] 
                                    if obj['Key'].endswith('.parquet')]
                        if part_files:
                            first_part = part_files[0]
                            logger.info(f"üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π part-—Ñ–∞–π–ª: {first_part}")
                            temp_file = "/tmp/fraud_data.parquet"
                            self.s3_client.download_file(bucket_name, first_part, temp_file)
                            df = pd.read_parquet(temp_file)
                            
                            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                            cleanup_temp = os.getenv("CLEANUP_TEMP_FILES", "true").lower() == "true"
                            if cleanup_temp:
                                os.remove(temp_file)
                                logger.info("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
                        else:
                            raise ValueError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã parquet —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
                    else:
                        raise ValueError("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞—è –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyArrow –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ dataset
                    logger.info("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ dataset —á–µ—Ä–µ–∑ PyArrow...")
                    
                    # –°–æ–∑–¥–∞–µ–º S3 filesystem –¥–ª—è PyArrow
                    s3_fs = fs.S3FileSystem(
                        endpoint_override=self.config.s3_endpoint_url.replace('http://', '').replace('https://', ''),
                        access_key=self.config.s3_access_key,
                        secret_key=self.config.s3_secret_key,
                        scheme='http'
                    )
                    
                    # –ß–∏—Ç–∞–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset
                    dataset_path = f"{bucket_name}/{key}"
                    dataset = pq.ParquetDataset(dataset_path, filesystem=s3_fs)
                    table = dataset.read()
                    df = table.to_pandas()
                    
                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å—Ç—Ä–æ–∫ –∏–∑ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ dataset")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–º–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def load_transaction_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö –∏–∑ S3 MinIO
        
        Returns:
            DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
        """
        data_path = self.config.data_path
        max_records = self.config.max_records
        
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3: {data_path}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—É—Ç—å S3
            if not data_path.startswith('s3a://'):
                raise ValueError(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ S3 –ø—É—Ç—å (s3a://), –ø–æ–ª—É—á–µ–Ω: {data_path}")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º s3a:// –≤ s3://
            s3_path = data_path.replace('s3a://', 's3://')
            bucket_name = s3_path.split('/')[2]
            key = '/'.join(s3_path.split('/')[3:])
            
            logger.info(f"üì° S3 –∑–∞–≥—Ä—É–∑–∫–∞: bucket={bucket_name}, key={key}")
            
            # –£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ –æ–±—ã—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, —Ç–∞–∫ –∏ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            df = self._load_data_smart(bucket_name, key)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if max_records and len(df) > max_records:
                df = df.sample(n=max_records, random_state=42)
                logger.info(f"üìä –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_records} –∑–∞–ø–∏—Å–µ–π")
            
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ S3")
            logger.info(f"üìã –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            if 'tx_fraud' in df.columns:
                fraud_count = df['tx_fraud'].sum()
                fraud_pct = (fraud_count / len(df)) * 100
                logger.info(f"üîç –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {fraud_count:,} ({fraud_pct:.1f}%)")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3: {e}")
            raise
    
    def prepare_transaction_message(self, row: pd.Series, timestamp: datetime) -> Dict[str, Any]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Kafka
        
        Args:
            row: –°—Ç—Ä–æ–∫–∞ –∏–∑ DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º pandas Series –≤ dict, –∏—Å–∫–ª—é—á–∞—è NaN –∑–Ω–∞—á–µ–Ω–∏—è
        transaction = {}
        for key, value in row.items():
            if pd.notna(value):  # –ò—Å–∫–ª—é—á–∞–µ–º NaN
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy —Ç–∏–ø—ã –≤ Python —Ç–∏–ø—ã –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                if hasattr(value, 'item'):
                    transaction[key] = value.item()
                else:
                    transaction[key] = value
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Ç–æ–∫–∞
        message = {
            "transaction_id": f"tx_{int(time.time() * 1000000)}_{self.stats.messages_sent}",
            "stream_timestamp": timestamp.isoformat(),
            "producer_id": "producer_replay_transactions",
            "producer_config": {
                "tps_target": self.config.tps_target,
                "topic": self.config.kafka_topic
            },
            "data": transaction
        }
        
        return message
    
    def send_transaction(self, message: Dict[str, Any]) -> bool:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ Kafka
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º transaction_id –∫–∞–∫ key –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            key = message["transaction_id"]
            
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
            future = self.producer.send(
                topic=self.config.kafka_topic,
                key=key,
                value=message
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–±–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ —Å timeout)
            record_metadata = future.get(timeout=self.config.timeout)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats.messages_sent += 1
            self.stats.bytes_sent += len(json.dumps(message).encode('utf-8'))
            
            return True
            
        except kafka_errors.KafkaTimeoutError:
            logger.warning("‚è∞ Kafka timeout –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è")
            self.stats.messages_error += 1
            return False
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Kafka: {e}")
            self.stats.messages_error += 1
            return False
    
    def calculate_stats(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        elapsed = (datetime.now() - self.stats.start_time).total_seconds()
        
        if elapsed > 0:
            self.stats.avg_tps = self.stats.messages_sent / elapsed
            
            # –í—ã—á–∏—Å–ª—è–µ–º TPS –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª
            if hasattr(self, '_last_stats_time') and hasattr(self, '_last_messages_count'):
                time_diff = time.time() - self._last_stats_time
                msg_diff = self.stats.messages_sent - self._last_messages_count
                if time_diff > 0:
                    self.stats.last_tps = msg_diff / time_diff
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
            self._last_stats_time = time.time()
            self._last_messages_count = self.stats.messages_sent
    
    def print_stats(self, total_records: int):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.config.enable_performance_logging:
            return
            
        self.calculate_stats()
        
        elapsed = (datetime.now() - self.stats.start_time).total_seconds()
        progress = (self.stats.messages_sent / total_records) * 100 if total_records > 0 else 0
        success_rate = (self.stats.messages_sent / (self.stats.messages_sent + self.stats.messages_error) * 100 
                       if (self.stats.messages_sent + self.stats.messages_error) > 0 else 0)
        
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] üìä Producer Statistics:")
        print(f"  ‚îú‚îÄ‚îÄ TPS Target: {self.config.tps_target} | Current: {self.stats.last_tps:.1f} | Average: {self.stats.avg_tps:.1f}")
        print(f"  ‚îú‚îÄ‚îÄ Messages: {self.stats.messages_sent:,} sent | {self.stats.messages_error} errors | {success_rate:.1f}% success")
        print(f"  ‚îú‚îÄ‚îÄ Progress: {self.stats.messages_sent:,}/{total_records:,} ({progress:.1f}%)")
        print(f"  ‚îú‚îÄ‚îÄ Topic: {self.config.kafka_topic}")
        print(f"  ‚îú‚îÄ‚îÄ Data Size: {self.stats.bytes_sent / 1024 / 1024:.1f} MB")
        print(f"  ‚îî‚îÄ‚îÄ Runtime: {elapsed:.1f}s")
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if self.stats.last_tps < self.config.tps_target * 0.9:
            print(f"  ‚ö†Ô∏è  WARNING: TPS below target ({self.stats.last_tps:.1f} < {self.config.tps_target})")
        if self.stats.messages_error > 10:
            print(f"  üö® ALERT: High error count ({self.stats.messages_error})")
    
    def replay_transactions(self) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è replay —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º TPS
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ Producer Replay: TPS={self.config.tps_target}, Topic={self.config.kafka_topic}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = self.load_transaction_data()
        total_records = len(df)
        
        if total_records == 0:
            error_msg = "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"
            logger.error(error_msg)
            return {"status": "error", "message": error_msg}
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è TPS
        target_interval = 1.0 / self.config.tps_target  # –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        self.running = True
        stats_interval = self.config.stats_interval
        last_stats_time = time.time()
        
        logger.info(f"üì° –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É {total_records:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å TPS={self.config.tps_target}")
        logger.info(f"‚è±Ô∏è  –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏: {target_interval*1000:.1f}ms")
        
        try:
            for idx, (_, row) in enumerate(df.iterrows()):
                if not self.running:
                    break
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                current_time = datetime.now()
                message = self.prepare_transaction_message(row, current_time)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
                send_start = time.time()
                success = self.send_transaction(message)
                send_duration = time.time() - send_start
                
                # –ö–æ–Ω—Ç—Ä–æ–ª—å TPS - –∂–¥–µ–º –Ω—É–∂–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                sleep_time = max(0, target_interval - send_duration)
                if sleep_time > 0:
                    time.sleep(sleep_time)
                
                # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if time.time() - last_stats_time >= stats_interval:
                    self.print_stats(total_records)
                    last_stats_time = time.time()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info("‚úÖ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            self.print_stats(total_records)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            result = self.get_final_report(total_records)
            
            return result
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C")
            self.running = False
            return {"status": "interrupted", "message": "Stopped by user"}
        
        except Exception as e:
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
            logger.error(error_msg)
            return {"status": "error", "message": error_msg}
        
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º producer
            if self.producer:
                self.producer.flush()
                self.producer.close()
                logger.info("‚úÖ Kafka Producer –∑–∞–∫—Ä—ã—Ç")
    
    def get_final_report(self, total_records: int) -> Dict[str, Any]:
        """–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        elapsed = (datetime.now() - self.stats.start_time).total_seconds()
        success_rate = (self.stats.messages_sent / (self.stats.messages_sent + self.stats.messages_error) * 100 
                       if (self.stats.messages_sent + self.stats.messages_error) > 0 else 0)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        tps_achieved = self.stats.avg_tps >= self.config.tps_target * 0.95
        reliability_ok = success_rate >= 99
        
        report = {
            "status": "success",
            "config": {
                "target_tps": self.config.tps_target,
                "total_records": total_records,
                "kafka_topic": self.config.kafka_topic,
                "data_path": self.config.data_path
            },
            "results": {
                "messages_sent": self.stats.messages_sent,
                "messages_error": self.stats.messages_error,
                "success_rate": round(success_rate, 1),
                "avg_tps": round(self.stats.avg_tps, 1),
                "data_size_mb": round(self.stats.bytes_sent / 1024 / 1024, 1),
                "runtime_seconds": round(elapsed, 1)
            },
            "analysis": {
                "tps_achieved": tps_achieved,
                "reliability_ok": reliability_ok,
                "overall_success": tps_achieved and reliability_ok
            }
        }
        
        if self.config.enable_performance_logging:
            print("\n" + "="*60)
            print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
            print("="*60)
            print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞:")
            print(f"  ‚îú‚îÄ‚îÄ Target TPS: {self.config.tps_target}")
            print(f"  ‚îú‚îÄ‚îÄ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
            print(f"  ‚îú‚îÄ‚îÄ Topic: {self.config.kafka_topic}")
            print(f"  ‚îî‚îÄ‚îÄ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed:.1f}s")
            print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            print(f"  ‚îú‚îÄ‚îÄ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {self.stats.messages_sent:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
            print(f"  ‚îú‚îÄ‚îÄ –û—à–∏–±–æ–∫: {self.stats.messages_error}")
            print(f"  ‚îú‚îÄ‚îÄ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
            print(f"  ‚îú‚îÄ‚îÄ –°—Ä–µ–¥–Ω–∏–π TPS: {self.stats.avg_tps:.1f}")
            print(f"  ‚îî‚îÄ‚îÄ –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {self.stats.bytes_sent / 1024 / 1024:.1f} MB")
            
            print(f"\n–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            if tps_achieved:
                print(f"  ‚úÖ –¶–µ–ª—å TPS –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ ({self.stats.avg_tps:.1f} ‚â• {self.config.tps_target * 0.95:.1f})")
            else:
                print(f"  ‚ö†Ô∏è  –¶–µ–ª—å TPS –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ ({self.stats.avg_tps:.1f} < {self.config.tps_target * 0.95:.1f})")
            
            if reliability_ok:
                print(f"  ‚úÖ –í—ã—Å–æ–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å ({success_rate:.1f}% —É—Å–ø–µ—à–Ω—ã—Ö)")
            else:
                print(f"  ‚ö†Ô∏è  –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é ({success_rate:.1f}% —É—Å–ø–µ—à–Ω—ã—Ö)")
        
        return report


# === –§–£–ù–ö–¶–ò–ò –î–õ–Ø AIRFLOW DAG ===

def run_producer_test(tps: int = 50, topic: str = "transactions-input-test", max_records: int = 1000) -> Dict[str, Any]:
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ producer'–∞ –∏–∑ Airflow DAG
    
    Args:
        tps: –¶–µ–ª–µ–≤–æ–π TPS
        topic: Kafka —Ç–æ–ø–∏–∫ 
        max_records: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ producer test: TPS={tps}, Topic={topic}, MaxRecords={max_records}")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ Airflow Variables
        config = get_config_from_airflow_variables(
            tps_override=tps,
            topic_override=topic,
            max_records_override=max_records
        )
        
        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º producer
        producer = TransactionProducer(config)
        result = producer.replay_transactions()
        
        logger.info(f"‚úÖ Producer test –∑–∞–≤–µ—Ä—à–µ–Ω: {result['status']}")
        return result
        
    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ producer test: {e}"
        logger.error(error_msg)
        return {"status": "error", "message": error_msg}


def run_producer_tps_50(**context) -> str:
    """TPS=50 —Å–æ–≥–ª–∞—Å–Ω–æ –≠—Ç–∞–ø—É 1 (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å tps=10)"""
    result = run_producer_test(tps=50, topic="transactions-input", max_records=1000)
    if result["status"] == "success":
        print(f"‚úÖ TPS=50: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ {result['results']['avg_tps']:.1f} avg TPS")
        return "success"
    else:
        raise Exception(f"TPS=50 test failed: {result.get('message', 'Unknown error')}")


def run_producer_small_load(**context) -> str:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –º–∞–ª–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è Airflow PythonOperator) - DEPRECATED, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ run_producer_tps_50"""
    return run_producer_tps_50(**context)


def run_producer_tps_100(**context) -> str:  
    """TPS=100 —Å–æ–≥–ª–∞—Å–Ω–æ –≠—Ç–∞–ø—É 1"""
    result = run_producer_test(tps=100, topic="transactions-input", max_records=1500)
    if result["status"] == "success":
        print(f"‚úÖ TPS=100: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ {result['results']['avg_tps']:.1f} avg TPS")
        return "success"
    else:
        raise Exception(f"TPS=100 test failed: {result.get('message', 'Unknown error')}")


def run_producer_medium_load(**context) -> str:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ä–µ–¥–Ω–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è Airflow PythonOperator) - DEPRECATED, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ run_producer_tps_100"""  
    return run_producer_tps_100(**context)


def run_producer_tps_200(**context) -> str:
    """TPS=200 —Å–æ–≥–ª–∞—Å–Ω–æ –≠—Ç–∞–ø—É 1"""
    result = run_producer_test(tps=200, topic="transactions-input", max_records=2000) 
    if result["status"] == "success":
        print(f"‚úÖ TPS=200: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ {result['results']['avg_tps']:.1f} avg TPS")
        return "success"
    else:
        raise Exception(f"TPS=200 test failed: {result.get('message', 'Unknown error')}")


def run_producer_tps_400(**context) -> str:
    """TPS=400 —Å–æ–≥–ª–∞—Å–Ω–æ –≠—Ç–∞–ø—É 1 (PEAK LOAD)"""  
    result = run_producer_test(tps=400, topic="transactions-input", max_records=3000)
    if result["status"] == "success":
        print(f"üöÄ TPS=400 PEAK: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ {result['results']['avg_tps']:.1f} avg TPS")
        return "success"
    else:
        raise Exception(f"TPS=400 test failed: {result.get('message', 'Unknown error')}")


def run_producer_high_load(**context) -> str:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è Airflow PythonOperator) - DEPRECATED, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ run_producer_tps_400"""
    return run_producer_tps_400(**context)


def run_escalating_attack(**context) -> str:
    """
    üö® ESCALATING ATTACK –¥–ª—è Tasks10 - –ò—Ç–µ—Ä–∞—Ü–∏—è 5
    –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏: 50‚Üí200‚Üí500‚Üí1500 TPS
    –¶–µ–ª—å: –¥–æ–≤–µ—Å—Ç–∏ ML API –¥–æ 6 –ø–æ–¥–æ–≤ + CPU>80% ‚Üí –∞–ª–µ—Ä—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞
    """
    logger.info("üö® –ó–∞–ø—É—Å–∫ ESCALATING ATTACK –¥–ª—è Tasks10!")
    
    # –§–∞–∑—ã –∞—Ç–∞–∫–∏ —Å —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–µ–π—Å—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ù–ê–ì–†–£–ó–ö–ê
    attack_phases = [
        {"name": "–§–∞–∑–∞ 1: –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç", "tps": 500, "duration_sec": 60, "records": 30000},
        {"name": "–§–∞–∑–∞ 2: –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∞—è –∞—Ç–∞–∫–∞", "tps": 1000, "duration_sec": 120, "records": 120000},
        {"name": "–§–∞–∑–∞ 3: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ê–¢–ê–ö–ê", "tps": 2000, "duration_sec": 180, "records": 360000},
        {"name": "–§–∞–∑–∞ 4: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ê–¢–ê–ö–ê", "tps": 5000, "duration_sec": 300, "records": 1500000}
    ]
    
    results = []
    
    for phase in attack_phases:
        logger.info(f"üî• {phase['name']}: {phase['tps']} TPS –Ω–∞ {phase['duration_sec']} —Å–µ–∫—É–Ω–¥")
        print(f"\n{'='*60}")
        print(f"üî• {phase['name']}")
        print(f"   TPS: {phase['tps']}")
        print(f"   –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {phase['duration_sec']} —Å–µ–∫—É–Ω–¥")
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–µ –∑–∞–ø–∏—Å–∏: {phase['records']:,}")
        print(f"{'='*60}")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–∞–∑—É –∞—Ç–∞–∫–∏
            result = run_producer_test(
                tps=phase['tps'], 
                topic="transactions-input", 
                max_records=phase['records']
            )
            
            if result['status'] == 'success':
                actual_tps = result['results']['avg_tps']
                logger.info(f"‚úÖ {phase['name']} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {actual_tps:.1f} TPS –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ")
                print(f"‚úÖ –§–∞–∑–∞ —É—Å–ø–µ—à–Ω–∞: {actual_tps:.1f} TPS")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã
                if phase != attack_phases[-1]:  # –ù–µ –∂–¥–µ–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ñ–∞–∑—ã
                    logger.info("‚è∏Ô∏è  –ü–∞—É–∑–∞ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã...")
                    print("‚è∏Ô∏è  –ü–∞—É–∑–∞ 30 —Å–µ–∫—É–Ω–¥...")
                    time.sleep(30)
                    
            else:
                logger.error(f"‚ùå {phase['name']} –ø—Ä–æ–≤–∞–ª–µ–Ω–∞: {result.get('message')}")
                print(f"‚ùå –§–∞–∑–∞ –ø—Ä–æ–≤–∞–ª–µ–Ω–∞: {result.get('message')}")
                
            results.append({
                "phase": phase['name'],
                "target_tps": phase['tps'],
                "actual_tps": result['results']['avg_tps'] if result['status'] == 'success' else 0,
                "status": result['status'],
                "message": result.get('message', 'OK')
            })
            
        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ {phase['name']}: {str(e)}")
            print(f"üí• –û—à–∏–±–∫–∞: {str(e)}")
            results.append({
                "phase": phase['name'],
                "target_tps": phase['tps'],
                "actual_tps": 0,
                "status": "error",
                "message": str(e)
            })
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\n{'='*60}")
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ ESCALATING ATTACK")
    print(f"{'='*60}")
    
    total_success = 0
    for result in results:
        status_icon = "‚úÖ" if result['status'] == 'success' else "‚ùå"
        print(f"{status_icon} {result['phase']}: {result['actual_tps']:.1f}/{result['target_tps']} TPS")
        if result['status'] == 'success':
            total_success += 1
    
    attack_success = total_success >= 3  # –ú–∏–Ω–∏–º—É–º 3 –∏–∑ 4 —Ñ–∞–∑ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏
    
    print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞—Ç–∞–∫–∏: {total_success}/4 —Ñ–∞–∑ —É—Å–ø–µ—à–Ω—ã")
    if attack_success:
        print("üö® ESCALATING ATTACK –£–°–ü–ï–®–ù–ê! –û–∂–∏–¥–∞–π—Ç–µ –∞–ª–µ—Ä—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ 5 –º–∏–Ω—É—Ç –ø—Ä–∏ CPU>80% + 6 –ø–æ–¥–æ–≤!")
        logger.info("üö® ESCALATING ATTACK –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ - —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è")
    else:
        print("‚ö†Ô∏è  ESCALATING ATTACK —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–æ–≤–∞–ª–µ–Ω–∞ - –≤–æ–∑–º–æ–∂–Ω–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è")
        logger.warning("‚ö†Ô∏è  ESCALATING ATTACK —á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–æ–≤–∞–ª–µ–Ω–∞")
    
    return f"ESCALATING ATTACK: {total_success}/4 —Ñ–∞–∑ —É—Å–ø–µ—à–Ω—ã - {'–£–°–ü–ï–•' if attack_success else '–ß–ê–°–¢–ò–ß–ù–´–ô –ü–†–û–í–ê–õ'}"


# === LEGACY MAIN –î–õ–Ø STANDALONE –ó–ê–ü–£–°–ö–ê ===

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è standalone –∑–∞–ø—É—Å–∫–∞ (—Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)"""
    logger.info("üöÄ Producer Replay Transactions - Standalone —Ä–µ–∂–∏–º")
    logger.info("‚ÑπÔ∏è –î–ª—è DAG –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ run_producer_*")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = get_config_from_airflow_variables()
        
        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º producer
        producer = TransactionProducer(config)
        result = producer.replay_transactions()
        
        if result["status"] == "success":
            logger.info("‚úÖ Producer –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.error(f"‚ùå Producer –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {result.get('message')}")
            return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())


# === AIRFLOW DAG –î–õ–Ø –≠–¢–ê–ü–ê 1 ===

try:
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ DAG
    default_args = {
        'owner': 'mlops-tasks8-etap1',
        'depends_on_past': False,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=2),
    }

    # –°–æ–∑–¥–∞–µ–º DAG
    dag = DAG(
        dag_id='tasks8_etap1_producer_test_v4',
        default_args=default_args,
        description='–≠—Ç–∞–ø 1: Producer —Å TPS 50,100,200,400',
        schedule=None,  # –¢–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π Airflow)
        start_date=datetime(2024, 8, 10),
        catchup=False,
        tags=['mlops', 'tasks8', 'etap1', 'producer', 'tps']
    )

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö TPS –∏–∑ –≠—Ç–∞–ø–∞ 1
    test_50_task = PythonOperator(
        task_id='test_tps_50', 
        python_callable=run_producer_tps_50,
        dag=dag
    )

    test_100_task = PythonOperator(
        task_id='test_tps_100',
        python_callable=run_producer_tps_100, 
        dag=dag
    )

    test_200_task = PythonOperator(
        task_id='test_tps_200',
        python_callable=run_producer_tps_200,
        dag=dag
    )

    test_400_task = PythonOperator(
        task_id='test_tps_400', 
        python_callable=run_producer_tps_400,
        dag=dag
    )

    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏
    test_50_task >> test_100_task >> test_200_task >> test_400_task

    # === DAG –î–õ–Ø TASKS10 ESCALATING ATTACK ===
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π DAG –¥–ª—è Tasks10 –ò—Ç–µ—Ä–∞—Ü–∏—è 5
    tasks10_dag = DAG(
        dag_id='tasks10_escalating_attack_v1',
        default_args={
            'owner': 'mlops-tasks10-iteration5',
            'depends_on_past': False,
            'email_on_failure': False,
            'email_on_retry': False,
            'retries': 1,
            'retry_delay': timedelta(minutes=5),
        },
        description='Tasks10 –ò—Ç–µ—Ä–∞—Ü–∏—è 5: Escalating Attack –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª–µ—Ä—Ç–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞',
        schedule=None,  # –¢–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
        start_date=datetime(2024, 12, 20),
        catchup=False,
        tags=['mlops', 'tasks10', 'iteration5', 'escalating-attack', 'kafka']
    )
    
    # –ó–∞–¥–∞—á–∞ escalating attack
    escalating_attack_task = PythonOperator(
        task_id='run_escalating_attack',
        python_callable=run_escalating_attack,
        dag=tasks10_dag,
        doc_md="""
        ## üö® ESCALATING ATTACK –¥–ª—è Tasks10
        
        **–¶–µ–ª—å**: –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–º–∏—Ç–∞—Ü–∏—é –∞—Ç–∞–∫–∏ —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º –Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏–µ–º –Ω–∞–≥—Ä—É–∑–∫–∏ 
        –¥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –∞–ª–µ—Ä—Ç–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ (6 —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ + CPU>80% –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç).
        
        **–§–∞–∑—ã –∞—Ç–∞–∫–∏**:
        1. **–§–∞–∑–∞ 1**: 50 TPS –Ω–∞ 60 —Å–µ–∫—É–Ω–¥ (–±–∞–∑–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞)
        2. **–§–∞–∑–∞ 2**: 200 TPS –Ω–∞ 120 —Å–µ–∫—É–Ω–¥ (–Ω–∞—á–∞–ª–æ HPA –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è)  
        3. **–§–∞–∑–∞ 3**: 500 TPS –Ω–∞ 180 —Å–µ–∫—É–Ω–¥ (4-5 –ø–æ–¥–æ–≤)
        4. **–§–∞–∑–∞ 4**: 1500 TPS –Ω–∞ 300 —Å–µ–∫—É–Ω–¥ (6 –ø–æ–¥–æ–≤ + CPU>80% ‚Üí üö® –ê–õ–ï–†–¢!)
        
        **–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: –°—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ `AdminNotification_MaxScaleHighCPU` 
        —á–µ—Ä–µ–∑ 5 –º–∏–Ω—É—Ç –ø–æ—Å–ª–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π.
        
        **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: 
        ```bash
        watch kubectl get pods -n mlops-tasks10
        kubectl top pods -n mlops-tasks10
        ```
        """
    )

except ImportError:
    # –ï—Å–ª–∏ Airflow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (standalone —Ä–µ–∂–∏–º), –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
    logger.info("‚ÑπÔ∏è Airflow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Ä–∞–±–æ—Ç–∞–µ–º –≤ standalone —Ä–µ–∂–∏–º–µ")
    dag = None
    tasks10_dag = None