"""
–®–ê–ì 4 (Tasks10): –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô Spark Streaming –¥–ª—è –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô HTTP –Ω–∞–≥—Ä—É–∑–∫–∏
–°–æ–∑–¥–∞–µ—Ç 5 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark Streaming jobs –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ ML API
"""

import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from spark_stream_inference_http import run_spark_streaming_http, test_ml_api_connection, cleanup_spark_checkpoints

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# DAG arguments
default_args = {
    'owner': 'mlops-tasks10-iteration5',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=35),
}

# –°–æ–∑–¥–∞–µ–º DAG –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark jobs
dag = DAG(
    dag_id='tasks10_spark_streaming_parallel_v1',
    default_args=default_args,
    description='Tasks10: 5 –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark Streaming jobs –¥–ª—è –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ ML API',
    schedule=None,
    start_date=datetime(2024, 12, 20),
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'tasks10', 'parallel-spark', 'maximum-load', 'v1']
)

# –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
test_api_task = PythonOperator(
    task_id='test_ml_api_connection',
    python_callable=test_ml_api_connection,
    dag=dag
)

# –û—á–∏—Å—Ç–∫–∞ checkpoints
cleanup_task = PythonOperator(
    task_id='cleanup_spark_checkpoints',
    python_callable=cleanup_spark_checkpoints,
    dag=dag
)

# –°–æ–∑–¥–∞–µ–º 5 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark Streaming tasks
parallel_tasks = []
for i in range(1, 6):
    task = PythonOperator(
        task_id=f'spark_streaming_job_{i}',
        python_callable=run_spark_streaming_http,
        dag=dag,
        doc_md=f"""
        ## üöÄ Parallel Spark Streaming Job {i}/5
        
        –û–¥–∏–Ω –∏–∑ 5 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark Streaming jobs –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ ML API.
        –û–±—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞: 5x Spark jobs = 5x HTTP requests/sec –∫ ML API
        """
    )
    parallel_tasks.append(task)

# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
test_api_task >> cleanup_task
for task in parallel_tasks:
    cleanup_task >> task

dag.doc_md = """
# üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –ê–¢–ê–ö–ê: 5x Spark Streaming Jobs

## üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è
–ó–∞–ø—É—Å–∫–∞–µ—Ç **5 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Spark Streaming jobs**, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö:
- –ß–∏—Ç–∞–µ—Ç –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ `transactions-input`
- –î–µ–ª–∞–µ—Ç HTTP POST –∑–∞–ø—Ä–æ—Å—ã –∫ ML API
- –°–æ–∑–¥–∞–µ—Ç **5x –Ω–∞–≥—Ä—É–∑–∫—É** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–Ω–∏–º job

## üìä –û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç
- **–ù–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ ML API**: 5x —É–≤–µ–ª–∏—á–µ–Ω–∏–µ HTTP requests/sec
- **CPU utilization**: –î–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—Å–∏—Ç—å 80% –æ—Ç requests
- **HPA scaling**: –ë—ã—Å—Ç—Ä–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ 6 –ø–æ–¥–æ–≤
- **Alert trigger**: –°—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞

## ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï
–≠—Ç–æ—Ç DAG —Å–æ–∑–¥–∞–µ—Ç –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–£–Æ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏—Å—Ç–µ–º—É!
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è HPA –∏ –∞–ª–µ—Ä—Ç–æ–≤.
"""
