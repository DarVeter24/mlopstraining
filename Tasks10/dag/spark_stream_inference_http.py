"""
–®–ê–ì 4 (Tasks10): Spark Streaming Job –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ ML API (ITERATION 5) - EXTENDED RUNTIME v7.0

üö® ESCALATING ATTACK: –≠—Ç–æ—Ç DAG –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –¥–ª—è Tasks10 Iteration 5
–í–º–µ—Å—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ inference –¥–µ–ª–∞–µ—Ç HTTP POST –∑–∞–ø—Ä–æ—Å—ã –∫ ML API –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏.

–≠—Ç–æ—Ç DAG —É–ø—Ä–∞–≤–ª—è–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –∫–æ—Ç–æ—Ä–æ–µ:
- –ß–∏—Ç–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ transactions-input
- ‚ùå –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ—Ç MLflow –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ
- ‚úÖ –î–µ–ª–∞–µ—Ç HTTP POST –∑–∞–ø—Ä–æ—Å—ã –∫ http://tasks10-ml-api.darveter.com/predict
- –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Kafka —Ç–æ–ø–∏–∫ fraud-predictions
- –°–æ–∑–¥–∞–µ—Ç –†–ï–ê–õ–¨–ù–£–Æ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ ML API –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è HPA –∏ –∞–ª–µ—Ä—Ç–æ–≤

üéØ –¶–ï–õ–¨: –î–æ–≤–µ—Å—Ç–∏ ML API –¥–æ 6 –ø–æ–¥–æ–≤ + CPU>80% ‚Üí üö® –ê–õ–ï–†–¢ –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–ê!

–ê–≤—Ç–æ—Ä: MLOps Task 10 - Iteration 5 - HTTP Attack Mode
"""

import logging
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import subprocess
import signal
import psutil

# Airflow imports
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.context import Context

# Spark imports (–±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark executors)
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    import requests  # ‚úÖ –ù–û–í–û–ï: –î–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ ML API
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    logging.warning("‚ö†Ô∏è Spark/requests –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Airflow - –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark cluster")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =====================================================================================
# üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø TASKS10 - HTTP MODE
# =====================================================================================

# üåê HTTP API Configuration
ML_API_URL = "http://tasks10-ml-service-service.mlops-tasks10.svc.cluster.local:80/predict"
ML_API_TIMEOUT = 10  # —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–ø—Ä–æ—Å
ML_API_RETRY_COUNT = 2

# üìä Kafka Configuration (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ Tasks8)
KAFKA_BOOTSTRAP_SERVERS = "kafka.kafka.svc.cluster.local:9092"
INPUT_TOPIC = "transactions-input"
OUTPUT_TOPIC = "fraud-predictions"

# üî• Spark Configuration –¥–ª—è HTTP —Ä–µ–∂–∏–º–∞
SPARK_CONFIG = {
    "spark.app.name": "Tasks10-HTTP-Fraud-Detection-Streaming",
    "spark.master": "local[*]",
    "spark.sql.streaming.checkpointLocation": "/tmp/spark-checkpoints-tasks10",
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.sql.streaming.forceDeleteTempCheckpointLocation": "true",
    
    # üöÄ Kafka Integration - CRITICAL FIX!
    "spark.jars.packages": "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5",
    
    # üöÄ HTTP –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    "spark.task.maxFailures": "3",
    "spark.sql.streaming.kafka.consumer.pollTimeoutMs": "5000",
    "spark.sql.execution.arrow.pyspark.enabled": "true",
    "spark.sql.streaming.statefulOperator.checkCorrectness.enabled": "false",
}

# =====================================================================================
# üåê HTTP ML API CLIENT FUNCTIONS
# =====================================================================================

def make_ml_api_request(transaction_data: dict) -> dict:
    """
    –î–µ–ª–∞–µ—Ç HTTP POST –∑–∞–ø—Ä–æ—Å –∫ ML API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
    
    Args:
        transaction_data: –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
        
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–ª–∏ –æ—à–∏–±–∫–∞
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ML API - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
        transaction_fields = transaction_data.get("data", {})
        payload = {
            "transaction_id": str(transaction_fields.get("transaction_id")),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É!
            "customer_id": transaction_fields.get("customer_id"),
            "terminal_id": transaction_fields.get("terminal_id"),
            "tx_amount": transaction_fields.get("tx_amount"),
            "tx_time_seconds": transaction_fields.get("tx_time_seconds"),
            "tx_time_days": transaction_fields.get("tx_time_days"),
            "tx_fraud_scenario": transaction_fields.get("tx_fraud_scenario")
        }
        
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "Tasks10-Spark-Streaming/1.0"
        }
        
        # –î–µ–ª–∞–µ–º HTTP POST –∑–∞–ø—Ä–æ—Å
        response = requests.post(
            ML_API_URL,
            json=payload,
            headers=headers,
            timeout=ML_API_TIMEOUT
        )
        
        if response.status_code == 200:
            result = response.json()
            return {
                "transaction_id": transaction_data.get("transaction_id", "unknown"),
                "prediction": result.get("predictions", [{}])[0],
                "http_status": response.status_code,
                "response_time_ms": response.elapsed.total_seconds() * 1000,
                "timestamp": datetime.now().isoformat(),
                "api_url": ML_API_URL,
                "success": True
            }
        else:
            return {
                "transaction_id": transaction_data.get("transaction_id", "unknown"),
                "error": f"HTTP {response.status_code}: {response.text}",
                "http_status": response.status_code,
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "api_url": ML_API_URL
            }
            
    except requests.exceptions.Timeout:
        return {
            "transaction_id": transaction_data.get("transaction_id", "unknown"),
            "error": f"Request timeout after {ML_API_TIMEOUT}s",
            "success": False,
            "timestamp": datetime.now().isoformat(),
            "api_url": ML_API_URL
        }
    except requests.exceptions.RequestException as e:
        return {
            "transaction_id": transaction_data.get("transaction_id", "unknown"),
            "error": f"Request failed: {str(e)}",
            "success": False,
            "timestamp": datetime.now().isoformat(),
            "api_url": ML_API_URL
        }
    except Exception as e:
        return {
            "transaction_id": transaction_data.get("transaction_id", "unknown"),
            "error": f"Unexpected error: {str(e)}",
            "success": False,
            "timestamp": datetime.now().isoformat(),
            "api_url": ML_API_URL
        }

def batch_http_inference(batch_df, batch_id):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç batch –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka, –¥–µ–ª–∞—è HTTP –∑–∞–ø—Ä–æ—Å—ã –∫ ML API
    
    Args:
        batch_df: Spark DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
        batch_id: ID –±–∞—Ç—á–∞
    """
    logger.info(f"üî• Processing batch {batch_id} with {batch_df.count()} transactions")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ batch
    transactions = batch_df.collect()
    results = []
    
    start_time = time.time()
    success_count = 0
    error_count = 0
    
    for row in transactions:
        try:
            # –ü–∞—Ä—Å–∏–º JSON –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            transaction_data = json.loads(row.value)
            
            # –î–µ–ª–∞–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∫ ML API
            result = make_ml_api_request(transaction_data)
            
            if result["success"]:
                success_count += 1
            else:
                error_count += 1
                logger.warning(f"‚ùå ML API error for transaction {result['transaction_id']}: {result.get('error')}")
            
            results.append(result)
            
        except json.JSONDecodeError as e:
            error_count += 1
            logger.error(f"‚ùå JSON parse error: {e}")
            results.append({
                "error": f"JSON parse error: {str(e)}",
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "raw_value": str(row.value)
            })
        except Exception as e:
            error_count += 1
            logger.error(f"‚ùå Processing error: {e}")
            results.append({
                "error": f"Processing error: {str(e)}",
                "success": False,
                "timestamp": datetime.now().isoformat()
            })
    
    processing_time = time.time() - start_time
    
    logger.info(f"""
    üìä Batch {batch_id} completed:
    ‚úÖ Successful HTTP requests: {success_count}
    ‚ùå Failed HTTP requests: {error_count}
    ‚è±Ô∏è Total processing time: {processing_time:.2f}s
    üåê ML API URL: {ML_API_URL}
    üìà Average time per request: {processing_time/len(transactions):.3f}s
    """)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤—ã—Ö–æ–¥–Ω–æ–π Kafka —Ç–æ–ø–∏–∫
    if results:
        try:
            # –°–æ–∑–¥–∞–µ–º Spark DataFrame –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            spark = SparkSession.getActiveSession()
            results_rdd = spark.sparkContext.parallelize(results)
            results_df = spark.read.json(results_rdd)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
            results_df.selectExpr("to_json(struct(*)) AS value") \
                .write \
                .format("kafka") \
                .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
                .option("topic", OUTPUT_TOPIC) \
                .save()
                
            logger.info(f"‚úÖ Sent {len(results)} results to Kafka topic {OUTPUT_TOPIC}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to send results to Kafka: {e}")

# =====================================================================================
# üöÄ SPARK STREAMING APPLICATION
# =====================================================================================

def run_spark_streaming_http(**context) -> str:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ ML API
    
    Returns:
        str: –°—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    logger.info("üöÄ Starting Spark Streaming HTTP ML API client...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º Spark Session
        spark = SparkSession.builder \
            .appName(SPARK_CONFIG["spark.app.name"])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        for key, value in SPARK_CONFIG.items():
            if key != "spark.app.name":
                spark = spark.config(key, value)
        
        spark = spark.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")
        
        logger.info(f"‚úÖ Spark Session created: {spark.version}")
        logger.info(f"üåê ML API URL: {ML_API_URL}")
        logger.info(f"üì• Input Kafka topic: {INPUT_TOPIC}")
        logger.info(f"üì§ Output Kafka topic: {OUTPUT_TOPIC}")
        
        # –ß–∏—Ç–∞–µ–º –∏–∑ Kafka
        kafka_df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
            .option("subscribe", INPUT_TOPIC) \
            .option("startingOffsets", "latest") \
            .option("failOnDataLoss", "false") \
            .load()
        
        logger.info("üì• Connected to Kafka input stream")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ–ª–∞–µ–º HTTP –∑–∞–ø—Ä–æ—Å—ã
        query = kafka_df \
            .writeStream \
            .foreachBatch(batch_http_inference) \
            .outputMode("append") \
            .option("checkpointLocation", SPARK_CONFIG["spark.sql.streaming.checkpointLocation"]) \
            .trigger(processingTime="10 seconds") \
            .start()
        
        logger.info("üî• Spark Streaming HTTP job started!")
        logger.info("üìä Monitoring ML API load generation...")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        start_time = time.time()
        max_runtime = 1800  # 30 –º–∏–Ω—É—Ç –¥–ª—è HPA —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!
        
        while query.isActive and (time.time() - start_time) < max_runtime:
            time.sleep(10)
            runtime_minutes = (time.time() - start_time) / 60
            progress = query.lastProgress
            if progress:
                logger.info(f"""
                üìà Streaming Progress (‚è±Ô∏è {runtime_minutes:.1f}min / {max_runtime/60:.0f}min):
                - Batch ID: {progress.get('batchId', 'N/A')}
                - Input rows/sec: {progress.get('inputRowsPerSecond', 0):.1f}
                - Processing time: {progress.get('durationMs', {}).get('triggerExecution', 0)}ms
                - üåê HTTP requests to ML API: {progress.get('inputRowsPerSecond', 0):.1f}/sec
                - üöÄ HPA —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ kubectl get hpa -n mlops-tasks10
                """)
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç—Ä–∏–º
        logger.info("üõë Stopping Spark Streaming job...")
        query.stop()
        query.awaitTermination(timeout=30)
        
        spark.stop()
        
        runtime = time.time() - start_time
        logger.info(f"‚úÖ Spark Streaming HTTP job completed successfully in {runtime:.1f}s")
        
        return f"SUCCESS: HTTP ML API load generation completed in {runtime:.1f}s"
        
    except Exception as e:
        logger.error(f"‚ùå Spark Streaming HTTP job failed: {e}")
        try:
            if 'spark' in locals():
                spark.stop()
        except:
            pass
        raise e

# =====================================================================================
# üîß HELPER FUNCTIONS
# =====================================================================================

def test_ml_api_connection(**context) -> str:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ML API"""
    logger.info(f"üß™ Testing ML API connection to {ML_API_URL}")
    
    try:
        # –¢–µ—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è
        test_transaction = {
            "transaction_id": "test_001",
            "amount": 100.0,
            "merchant_category": "grocery",
            "hour_of_day": 14,
            "day_of_week": 2,
            "is_weekend": False,
            "user_age": 30,
            "transaction_frequency": 1.5
        }
        
        result = make_ml_api_request(test_transaction)
        
        if result["success"]:
            logger.info(f"‚úÖ ML API test successful: {result}")
            return f"SUCCESS: ML API responding in {result.get('response_time_ms', 0):.1f}ms"
        else:
            logger.error(f"‚ùå ML API test failed: {result}")
            return f"FAILED: {result.get('error', 'Unknown error')}"
            
    except Exception as e:
        logger.error(f"‚ùå ML API test error: {e}")
        return f"ERROR: {str(e)}"

def cleanup_spark_checkpoints(**context) -> str:
    """–û—á–∏—â–∞–µ—Ç Spark checkpoints"""
    checkpoint_dir = SPARK_CONFIG["spark.sql.streaming.checkpointLocation"]
    try:
        import shutil
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
            logger.info(f"‚úÖ Cleaned up checkpoint directory: {checkpoint_dir}")
        return f"SUCCESS: Cleaned up {checkpoint_dir}"
    except Exception as e:
        logger.error(f"‚ùå Cleanup failed: {e}")
        return f"ERROR: {str(e)}"

# =====================================================================================
# üéØ AIRFLOW DAG DEFINITION
# =====================================================================================

# DAG arguments
default_args = {
    'owner': 'mlops-tasks10-iteration5',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=35),  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è 30-–º–∏–Ω—É—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
}

# –°–æ–∑–¥–∞–µ–º DAG
dag = DAG(
    dag_id='tasks10_spark_streaming_http_v7',
    default_args=default_args,
    description='Tasks10 Iteration 5: Spark Streaming HTTP ML API Load Generator v7 - Extended runtime for HPA testing (30min)',
    schedule=None,  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é –∏–ª–∏ —á–µ—Ä–µ–∑ escalating attack
    start_date=datetime(2024, 12, 20),
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'tasks10', 'iteration5', 'spark-streaming', 'http-api', 'load-generation', 'v7', 'extended-runtime-30min']
)

# Task 1: –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ML API
test_api_task = PythonOperator(
    task_id='test_ml_api_connection',
    python_callable=test_ml_api_connection,
    dag=dag,
    doc_md="""
    ## üß™ Test ML API Connection
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ML API –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    
    **–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è:**
    - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å http://tasks10-ml-api.darveter.com/predict
    - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ API
    - –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
    
    **–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** HTTP 200 —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
    """
)

# Task 2: –û—á–∏—Å—Ç–∫–∞ checkpoints
cleanup_task = PythonOperator(
    task_id='cleanup_spark_checkpoints',
    python_callable=cleanup_spark_checkpoints,
    dag=dag,
    doc_md="""
    ## üßπ Cleanup Spark Checkpoints
    
    –û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ Spark Streaming checkpoints –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞.
    """
)

# Task 3: –û—Å–Ω–æ–≤–Ω–æ–π Spark Streaming HTTP job
streaming_task = PythonOperator(
    task_id='run_spark_streaming_http',
    python_callable=run_spark_streaming_http,
    dag=dag,
    doc_md="""
    ## üöÄ Spark Streaming HTTP ML API Load Generator
    
    **–ì–õ–ê–í–ù–ê–Ø –ó–ê–î–ê–ß–ê ITERATION 5:**
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTTP –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ ML API –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è HPA –∏ –∞–ª–µ—Ä—Ç–æ–≤.
    
    **–ü—Ä–æ—Ü–µ—Å—Å:**
    1. –ß–∏—Ç–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ `transactions-input`
    2. –î–ª—è –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–µ–ª–∞–µ—Ç HTTP POST –∫ ML API
    3. –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–æ–ø–∏–∫ `fraud-predictions`
    4. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ—à–∏–±–∫–∏
    
    **–¶–µ–ª—å:** –î–æ–≤–µ—Å—Ç–∏ ML API –¥–æ 6 –ø–æ–¥–æ–≤ + CPU>80% ‚Üí üö® –ê–õ–ï–†–¢ –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–ê!
    
    **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
    - ML API: http://tasks10-ml-api.darveter.com/predict
    - Timeout: 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–ø—Ä–æ—Å
    - Retry: 2 –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    - Batch interval: 10 —Å–µ–∫—É–Ω–¥
    """
)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
test_api_task >> cleanup_task >> streaming_task

# =====================================================================================
# üéØ DOCUMENTATION
# =====================================================================================

dag.doc_md = """
# üö® Tasks10 Iteration 5: HTTP ML API Load Generator

## üéØ –¶–µ–ª—å
–°–æ–∑–¥–∞—Ç—å **—Ä–µ–∞–ª—å–Ω—É—é HTTP –Ω–∞–≥—Ä—É–∑–∫—É** –Ω–∞ ML API –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
- Horizontal Pod Autoscaler (HPA)
- AlertManager –ø—Ä–∞–≤–∏–ª–∞
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–µ—Ä—Ç `AdminNotification_MaxScaleHighCPU`

## üî• Escalating Attack Strategy
1. **Kafka Producer** –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–º TPS: 50‚Üí200‚Üí500‚Üí1500
2. **Spark Streaming** —á–∏—Ç–∞–µ—Ç –∏–∑ Kafka –∏ –¥–µ–ª–∞–µ—Ç HTTP POST –∫ ML API
3. **ML API** –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã ‚Üí CPU –Ω–∞–≥—Ä—É–∑–∫–∞ ‚Üí HPA –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
4. **Prometheus** –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ ‚Üí AlertManager ‚Üí üö® –ê–õ–ï–†–¢ –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–ê!

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- **Grafana Dashboard**: http://grafana.darveter.com/d/tasks10-ml-dashboard
- **ML API URL**: http://tasks10-ml-api.darveter.com/predict
- **Prometheus**: –ú–µ—Ç—Ä–∏–∫–∏ CPU, Memory, HTTP requests

## üö® –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
–ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ 1500 TPS:
- ML API –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –¥–æ 6 –ø–æ–¥–æ–≤ (–º–∞–∫—Å–∏–º—É–º)
- CPU usage > 80% –≤ —Ç–µ—á–µ–Ω–∏–µ 5+ –º–∏–Ω—É—Ç
- –°—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–ª–µ—Ä—Ç `AdminNotification_MaxScaleHighCPU`
- –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- **Spark Local Mode**: –û–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º
- **HTTP Client**: requests —Å timeout –∏ retry
- **Error Handling**: –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
- **Kafka Integration**: –í—Ö–æ–¥/–≤—ã—Ö–æ–¥ —á–µ—Ä–µ–∑ Kafka —Ç–æ–ø–∏–∫–∏
"""

if __name__ == "__main__":
    print("üöÄ Tasks10 Spark Streaming HTTP ML API Load Generator")
    print(f"üåê ML API URL: {ML_API_URL}")
    print(f"üì• Input topic: {INPUT_TOPIC}")
    print(f"üì§ Output topic: {OUTPUT_TOPIC}")
