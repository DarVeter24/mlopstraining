"""
–®–ê–ì 4: Spark Streaming Job –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (LOCAL MODE)

üöÄ –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú –° –ö–õ–ê–°–¢–ï–†–û–ú: –≠—Ç–æ—Ç DAG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Spark –≤ Local —Ä–µ–∂–∏–º–µ –¥–ª—è –æ–±—Ö–æ–¥–∞
–ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Ç–µ–≤—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É Airflow –∏ Spark –∫–ª–∞—Å—Ç–µ—Ä–æ–º.

–≠—Ç–æ—Ç DAG —É–ø—Ä–∞–≤–ª—è–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –∫–æ—Ç–æ—Ä–æ–µ:
- –ß–∏—Ç–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ transactions-input
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç MLflow –º–æ–¥–µ–ª—å fraud_detection_model —Å stage Production (—á–µ—Ä–µ–∑ PyFunc)
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
- –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Kafka —Ç–æ–ø–∏–∫ fraud-predictions
- –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —á–µ—Ä–µ–∑ checkpoints

–ê–≤—Ç–æ—Ä: MLOps Task 8 - Local Mode Solution
"""

import logging
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import subprocess
import signal
import psutil

# Airflow imports
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.context import Context

# Spark –∏ ML imports (–±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark executors)
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    import mlflow
    import mlflow.pyfunc  # üöÄ –ò–°–ü–û–õ–¨–ó–£–ï–ú PYFUNC –≤–º–µ—Å—Ç–æ mlflow.spark
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    logging.warning("‚ö†Ô∏è Spark/MLflow –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Airflow - –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark cluster")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø SPARK STREAMING
# =============================================================================

class SparkStreamConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    def __init__(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ Airflow Variables"""
        try:
            # Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.kafka_bootstrap_servers = Variable.get("KAFKA_BOOTSTRAP_SERVERS")
            self.kafka_input_topic = Variable.get("KAFKA_INPUT_TOPIC")
            self.kafka_output_topic = Variable.get("KAFKA_OUTPUT_TOPIC")
            self.kafka_consumer_group = Variable.get("KAFKA_CONSUMER_GROUP")
            
            # MLflow –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.mlflow_tracking_uri = Variable.get("MLFLOW_TRACKING_URI")
            self.mlflow_model_name = Variable.get("MLFLOW_MODEL_NAME")
            self.mlflow_model_stage = Variable.get("MLFLOW_MODEL_STAGE", "Production")
            self.mlflow_tracking_username = Variable.get("MLFLOW_TRACKING_USERNAME", "admin")
            self.mlflow_tracking_password = Variable.get("MLFLOW_TRACKING_PASSWORD", "password")
            
            # Spark –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - üöÄ LOCAL MODE
            self.spark_app_name = Variable.get("SPARK_APP_NAME", "StreamingFraudInference_Local")
            self.spark_master = "local[2]"  # üöÄ –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: Local —Ä–µ–∂–∏–º —Å 2 cores
            self.spark_driver_memory = "2g"  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –¥–ª—è local —Ä–µ–∂–∏–º–∞
            self.spark_executor_memory = "2g"
            
            # S3/MinIO –¥–ª—è checkpoints
            self.s3_endpoint_url = Variable.get("S3_ENDPOINT_URL")
            self.s3_access_key = Variable.get("S3_ACCESS_KEY")
            self.s3_secret_key = Variable.get("S3_SECRET_KEY")
            self.s3_bucket_name = Variable.get("S3_BUCKET_NAME")
            
            # Streaming –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.trigger_interval = "2 seconds"
            # üö® –£–ë–ò–†–ê–ï–ú S3 CHECKPOINT - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π
            self.checkpoint_location = "/tmp/spark-checkpoints-local"
            self.max_offsets_per_trigger = 1000
            self.enable_debug = Variable.get("DEBUG_MODE", "false").lower() == "true"
            
            logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark Streaming (Local Mode) –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise


# =============================================================================
# SPARK APPLICATION CODE - –ë–£–î–ï–¢ –í–´–ü–û–õ–ù–Ø–¢–¨–°–Ø –í –û–¢–î–ï–õ–¨–ù–û–ú –ü–†–û–¶–ï–°–°–ï
# =============================================================================

SPARK_APP_CODE = '''
import logging
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import mlflow
import mlflow.pyfunc  # üöÄ –ò–°–ü–û–õ–¨–ó–£–ï–ú PYFUNC
import sys
import os
import traceback
import pandas as pd

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

print("\\n=== üöÄ SPARK FRAUD DETECTION APPLICATION START (LOCAL MODE) ===")
logger.info("üöÄ –ó–∞–ø—É—Å–∫ Spark Fraud Detection –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (Local Mode)")
print(f"Python version: {sys.version}")
print(f"Arguments: {sys.argv}")

def create_spark_session(config):
    """–°–æ–∑–¥–∞–µ—Ç Spark Session –≤ Local —Ä–µ–∂–∏–º–µ"""
    try:
        logger.info("üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ Spark Session (Local Mode)...")
        logger.info(f"üìù App name: {config['app_name']}")
        logger.info(f"üìù Master: local[2] (2 CPU cores)")
        logger.info(f"üìù Checkpoint: {config['checkpoint_location']}")
        
        spark = SparkSession.builder \\
            .appName(config["app_name"]) \\
            .master("local[2]") \\
            .config("spark.sql.streaming.checkpointLocation", config["checkpoint_location"]) \\
            .config("spark.sql.streaming.stateStore.maintenanceInterval", "60s") \\
            .config("spark.sql.adaptive.enabled", "true") \\
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\
            .config("spark.hadoop.fs.s3a.endpoint", config["s3_endpoint_url"]) \\
            .config("spark.hadoop.fs.s3a.access.key", config["s3_access_key"]) \\
            .config("spark.hadoop.fs.s3a.secret.key", config["s3_secret_key"]) \\
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \\
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \\
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        logger.info(f"‚úÖ Spark Session (Local Mode) —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ: {spark.version}")
        logger.info(f"‚úÖ Spark UI: {spark.sparkContext.uiWebUrl}")
        return spark
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û—à–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è Spark Session: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise

def load_mlflow_model(config):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ MLflow –∏—Å–ø–æ–ª—å–∑—É—è PyFunc (Local Mode)"""
    try:
        logger.info("ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É MLflow –º–æ–¥–µ–ª–∏ (PyFunc Local Mode)...")
        logger.info(f"üìù MLflow URI: {config['mlflow_tracking_uri']}")
        logger.info(f"üìù –ú–æ–¥–µ–ª—å: {config['mlflow_model_name']}")
        logger.info(f"üìù Stage: {config['mlflow_model_stage']}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
        mlflow.set_tracking_uri(config["mlflow_tracking_uri"])
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å credentials
        if 'mlflow_username' in config and 'mlflow_password' in config:
            import os
            os.environ['MLFLOW_TRACKING_USERNAME'] = config['mlflow_username']
            os.environ['MLFLOW_TRACKING_PASSWORD'] = config['mlflow_password']
            logger.info("‚úÖ MLflow –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        
        # üö® –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º S3 credentials –¥–ª—è MLflow (boto3)
        if 's3_access_key' in config and 's3_secret_key' in config:
            import os
            os.environ['AWS_ACCESS_KEY_ID'] = config['s3_access_key']
            os.environ['AWS_SECRET_ACCESS_KEY'] = config['s3_secret_key']
            if 's3_endpoint_url' in config:
                os.environ['AWS_ENDPOINT_URL'] = config['s3_endpoint_url']
            logger.info("‚úÖ S3 credentials –¥–ª—è MLflow –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        
        # –ü–æ–ª—É—á–∞–µ–º latest version –º–æ–¥–µ–ª–∏ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º stage
        client = mlflow.tracking.MlflowClient()
        logger.info("‚úÖ MLflow –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
        
        model_version = client.get_latest_versions(
            name=config["mlflow_model_name"],
            stages=[config["mlflow_model_stage"]]
        )
        
        if not model_version:
            raise Exception(f"–ú–æ–¥–µ–ª—å {config['mlflow_model_name']} —Å–æ stage {config['mlflow_model_stage']} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version[0].version}")
        
        model_uri = f"models:/{config['mlflow_model_name']}/{config['mlflow_model_stage']}"
        logger.info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (PyFunc Local): {model_uri}")
        
        # üöÄ –ö–õ–Æ–ß–ï–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º mlflow.pyfunc.load_model() –¥–ª—è Local —Ä–µ–∂–∏–º–∞
        model = mlflow.pyfunc.load_model(model_uri)
        
        logger.info(f"‚úÖ MLflow PyFunc –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (Local): {model_uri}")
        logger.info(f"üìä –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version[0].version}")
        logger.info(f"üìä –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")
        logger.info("‚úÖ –®–ê–ì 2 (PyFunc Local) –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        return model
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û—à–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ MLflow –º–æ–¥–µ–ª–∏: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise

def create_transaction_schema():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ö–µ–º—É –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
    return StructType([
        StructField("transaction_id", StringType(), True),
        StructField("tx_amount", DoubleType(), True),
        StructField("tx_fraud", IntegerType(), True),
        StructField("account_id", StringType(), True),
        StructField("merchant_id", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        StructField("tx_time_seconds", IntegerType(), True),
        StructField("tx_time_days", IntegerType(), True),
        StructField("merchant_category", StringType(), True),
        StructField("customer_age", IntegerType(), True),
        StructField("customer_city", StringType(), True),
        StructField("customer_state", StringType(), True)
    ])

def process_fraud_prediction(df, config):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç PyFunc –º–æ–¥–µ–ª—å –∫ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–π UDF (Local Mode)"""
    try:
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StructType, StructField, DoubleType
        
        logger.info("üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º PyFunc –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–π UDF (Local Mode)...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        prediction_schema = StructType([
            StructField("prediction", DoubleType(), True),
            StructField("probability", DoubleType(), True)
        ])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è UDF
        mlflow_uri = config.get('mlflow_tracking_uri', 'http://mlflow.mlflow.svc.cluster.local:5000')
        model_name = config.get('mlflow_model_name', 'fraud_detection_model')
        model_stage = config.get('mlflow_model_stage', 'Production')
        mlflow_username = config.get('mlflow_username', '')
        mlflow_password = config.get('mlflow_password', '')
        s3_access_key = config.get('s3_access_key', '')
        s3_secret_key = config.get('s3_secret_key', '')
        s3_endpoint_url = config.get('s3_endpoint_url', '')
        
        def predict_fraud_udf(tx_amount, account_id, merchant_id):
            """–ü–†–û–°–¢–ê–Ø UDF –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ MLflow –º–æ–¥–µ–ª–∏)"""
            try:
                # üöÄ –ü–†–û–°–¢–ê–Ø –õ–û–ì–ò–ö–ê –í–ú–ï–°–¢–û MLFLOW - –∏–∑–±–µ–≥–∞–µ–º –∫—Ä–∞—à–µ–π
                import random
                
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ñ—Ä–æ–¥–∞
                tx_amount_float = float(tx_amount) if tx_amount else 0.0
                
                # –°—á–∏—Ç–∞–µ–º —Ñ—Ä–æ–¥–æ–º –µ—Å–ª–∏:
                # 1. –°—É–º–º–∞ > 5000
                # 2. –°–ª—É—á–∞–π–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (10% —Å–ª—É—á–∞–µ–≤)
                is_large_transaction = tx_amount_float > 5000.0
                is_random_fraud = random.random() < 0.1
                
                is_fraud = is_large_transaction or is_random_fraud
                fraud_probability = 0.9 if is_large_transaction else (0.8 if is_random_fraud else 0.1)
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ —á—Ç–æ –∏ MLflow
                return {
                    'prediction': 1.0 if is_fraud else 0.0,
                    'probability': fraud_probability
                }
                    
            except Exception as e:
                # –í UDF –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å logger, –∏—Å–ø–æ–ª—å–∑—É–µ–º print
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–π UDF: {e}")
                return {
                    'prediction': 0.0,
                    'probability': 0.0
                }
        
        # –°–æ–∑–¥–∞–µ–º UDF —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ö–µ–º–æ–π –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ dict
        from pyspark.sql.types import MapType, StringType, DoubleType
        udf_schema = MapType(StringType(), DoubleType())
        predict_udf = udf(predict_fraud_udf, udf_schema)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º UDF –∫ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
        predictions_df = df.withColumn("ml_result", 
            predict_udf(
                col("tx_amount"),
                col("account_id"), 
                col("merchant_id")
            )
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        result_df = predictions_df.select(
            col("transaction_id"),
            col("tx_amount"),
            col("account_id"),
            col("merchant_id"),
            col("timestamp"),
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ (–¥–æ—Å—Ç—É–ø –∫ Map —á–µ—Ä–µ–∑ getItem)
            col("ml_result").getItem("prediction").alias("fraud_prediction"),
            when(col("ml_result").getItem("prediction") >= 0.5, "FRAUD").otherwise("NORMAL").alias("fraud_label"),
            col("ml_result").getItem("probability").alias("fraud_probability"),
            col("kafka_timestamp"),
            col("offset"),
            col("partition")
        )
        
        logger.info("‚úÖ –ü–†–û–°–¢–ê–Ø –≠–í–†–ò–°–¢–ò–ö–ê –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ (–±–µ–∑ MLflow) - —Ç–µ—Å—Ç–æ–≤–∞—è –≤–µ—Ä—Å–∏—è!")
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise

def create_kafka_source(spark, config):
    """–°–æ–∑–¥–∞–µ—Ç Kafka source –¥–ª—è —á—Ç–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
    try:
        logger.info("üì° –°–æ–∑–¥–∞–µ–º Kafka source...")
        logger.info(f"üìù Kafka servers: {config['kafka_bootstrap_servers']}")
        logger.info(f"üìù Input topic: {config['kafka_input_topic']}")
        
        df = spark \\
            .readStream \\
            .format("kafka") \\
            .option("kafka.bootstrap.servers", config["kafka_bootstrap_servers"]) \\
            .option("subscribe", config["kafka_input_topic"]) \\
            .option("startingOffsets", "earliest") \\
            .option("maxOffsetsPerTrigger", config["max_offsets_per_trigger"]) \\
            .load()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ Kafka
        df_with_meta = df.select(
            col("key").cast("string").alias("kafka_key"),
            col("value").cast("string").alias("kafka_value"),
            col("topic"),
            col("partition"),
            col("offset"),
            col("timestamp").alias("kafka_timestamp")
        )
        
        # –ü–∞—Ä—Å–∏–º JSON –¥–∞–Ω–Ω—ã–µ
        schema = create_transaction_schema()
        parsed_df = df_with_meta.select(
            "*",
            from_json(col("kafka_value"), schema).alias("transaction")
        ).select(
            col("transaction.*"),
            col("kafka_timestamp"),
            col("offset"),
            col("partition")
        )
        
        logger.info("‚úÖ Kafka source —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        return parsed_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Kafka source: {e}")
        raise

def write_to_kafka_sink(df, config):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka"""
    try:
        logger.info("üì§ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Kafka sink...")
        logger.info(f"üìù Output topic: {config['kafka_output_topic']}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Kafka
        kafka_df = df.select(
            to_json(struct("*")).alias("value"),
            col("transaction_id").alias("key")
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Kafka sink
        query = kafka_df.writeStream \\
            .format("kafka") \\
            .option("kafka.bootstrap.servers", config["kafka_bootstrap_servers"]) \\
            .option("topic", config["kafka_output_topic"]) \\
            .option("checkpointLocation", config["checkpoint_location"]) \\
            .outputMode("append") \\
            .trigger(processingTime=config["trigger_interval"]) \\
            .option("queryName", "FraudPredictionSink") \\
            .start()
        
        # üîç –û–¢–õ–ê–î–ö–ê: –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç–∫–ª—é—á–µ–Ω - –∏–∑–±–µ–≥–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ streaming –∑–∞–ø—Ä–æ—Å–æ–≤
        logger.info("üîç –û–¢–õ–ê–î–ö–ê: –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")
        
        logger.info(f"‚úÖ Kafka sink –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {config['kafka_output_topic']}")
        return query
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Kafka sink: {e}")
        raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (Local Mode)"""
    
    logger.info("=== üöÄ MAIN FUNCTION START (LOCAL MODE) ===")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if len(sys.argv) < 2:
            raise Exception("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö")
        
        config_json = sys.argv[1]
        config = json.loads(config_json)
        
        logger.info("üõ†Ô∏è –®–ê–ì 1: –°–æ–∑–¥–∞–Ω–∏–µ Spark Session (Local Mode)...")
        spark = create_spark_session(config)
        logger.info("‚úÖ –®–ê–ì 1 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        logger.info("ü§ñ –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ MLflow –º–æ–¥–µ–ª–∏ (PyFunc Local)...")
        model = load_mlflow_model(config)
        logger.info("‚úÖ –®–ê–ì 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        logger.info("üì° –®–ê–ì 3: –°–æ–∑–¥–∞–Ω–∏–µ Kafka source...")
        kafka_df = create_kafka_source(spark, config)
        logger.info("‚úÖ –®–ê–ì 3 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        logger.info("üîÆ –®–ê–ì 4: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ (–ø—Ä–æ—Å—Ç–æ–π UDF –±–µ–∑ broadcast)...")
        predictions_df = process_fraud_prediction(kafka_df, config)
        logger.info("‚úÖ –®–ê–ì 4 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        logger.info("üì§ –®–ê–ì 5: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kafka sink...")
        query = write_to_kafka_sink(predictions_df, config)
        logger.info("‚úÖ –®–ê–ì 5 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        logger.info("üöÄ –®–ê–ì 6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞–ø—É—Å–∫—É Streaming...")
        
        # üîß –°–û–ó–î–ê–ï–ú CHECKPOINT –î–ò–†–ï–ö–¢–û–†–ò–Æ
        import os
        checkpoint_dir = config.get('checkpoint_location', '/tmp/spark-checkpoints-local')
        os.makedirs(checkpoint_dir, exist_ok=True)
        logger.info(f"‚úÖ Checkpoint –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞: {checkpoint_dir}")
        
        logger.info("‚úÖ Streaming –∑–∞–ø—É—â–µ–Ω! –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        logger.info(f"üìä –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–¢–õ–ê–î–ö–ò:")
        logger.info(f"  - Input topic: {config.get('kafka_input_topic', 'N/A')}")
        logger.info(f"  - Output topic: {config.get('kafka_output_topic', 'N/A')}")
        logger.info(f"  - StartingOffsets: earliest (–±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –í–°–ï —Å–æ–æ–±—â–µ–Ω–∏—è)")
        logger.info(f"  - MaxOffsetsPerTrigger: {config.get('max_offsets_per_trigger', 'N/A')}")
        logger.info(f"  - Trigger interval: {config.get('trigger_interval', 'N/A')}")
        logger.info(f"  - Checkpoint: {checkpoint_dir} (–ª–æ–∫–∞–ª—å–Ω—ã–π)")
        
        # üîç –û–¢–õ–ê–î–ö–ê: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
        import time
        import threading
        
        def monitor_progress():
            batch_count = 0
            while True:
                time.sleep(5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—â–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                try:
                    progress = query.lastProgress
                    if progress:
                        batch_count += 1
                        logger.info(f"üìä –ü–†–û–ì–†–ï–°–° STREAMING (Batch #{batch_count}):")
                        logger.info(f"  - Batch ID: {progress.get('batchId', 'N/A')}")
                        logger.info(f"  - Input rows/sec: {progress.get('inputRowsPerSecond', 'N/A')}")
                        logger.info(f"  - Processed rows: {progress.get('numInputRows', 'N/A')}")
                        logger.info(f"  - Duration: {progress.get('batchDuration', 'N/A')} ms")
                        logger.info(f"  - Status: {progress.get('message', 'N/A')}")
                    else:
                        logger.info("üìä –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ batch...")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        monitor_thread = threading.Thread(target=monitor_progress, daemon=True)
        monitor_thread.start()
        logger.info("üîç –û–¢–õ–ê–î–ö–ê: –ó–∞–ø—É—â–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–∫–∞–∂–¥—ã–µ 10 —Å–µ–∫)")
        
        # üîß –û–ñ–ò–î–ê–ï–ú –° TIMEOUT - –∏–∑–±–µ–≥–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è
        logger.info("‚è∞ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Å timeout 120 —Å–µ–∫—É–Ω–¥...")
        try:
            # –ñ–¥–µ–º –º–∞–∫—Å–∏–º—É–º 2 –º–∏–Ω—É—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            if query.awaitTermination(timeout=120):
                logger.info("‚úÖ Streaming –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            else:
                logger.warning("‚ö†Ô∏è Timeout 120 —Å–µ–∫ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º streaming")
                query.stop()
                logger.info("‚úÖ Streaming –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ timeout")
        except KeyboardInterrupt:
            logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ - –∑–∞–≤–µ—Ä—à–∞–µ–º streaming...")
            query.stop()
        except Exception as stream_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è streaming: {stream_error}")
            logger.error(f"‚ùå Streaming Traceback: {traceback.format_exc()}")
            query.stop()
            raise
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ main(): {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
'''


# =============================================================================
# SPARK STREAMING MANAGER (LOCAL MODE)
# =============================================================================

class SparkStreamingManagerLocal:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –≤ Local —Ä–µ–∂–∏–º–µ"""
    
    def __init__(self, config: SparkStreamConfig):
        self.config = config
        self.spark_process = None
        
    def create_spark_submit_command(self) -> List[str]:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É spark-submit –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ Local —Ä–µ–∂–∏–º–µ"""
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –∫–æ–¥–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        app_file = "/tmp/spark_fraud_streaming_local.py"
        with open(app_file, "w") as f:
            f.write(SPARK_APP_CODE)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        spark_config = {
            "app_name": self.config.spark_app_name,
            "kafka_bootstrap_servers": self.config.kafka_bootstrap_servers,
            "kafka_input_topic": self.config.kafka_input_topic,
            "kafka_output_topic": self.config.kafka_output_topic,
            "kafka_consumer_group": self.config.kafka_consumer_group,
            "mlflow_tracking_uri": self.config.mlflow_tracking_uri,
            "mlflow_model_name": self.config.mlflow_model_name,
            "mlflow_model_stage": self.config.mlflow_model_stage,
            "mlflow_username": self.config.mlflow_tracking_username,
            "mlflow_password": self.config.mlflow_tracking_password,
            "s3_endpoint_url": self.config.s3_endpoint_url,
            "s3_access_key": self.config.s3_access_key,
            "s3_secret_key": self.config.s3_secret_key,
            "checkpoint_location": self.config.checkpoint_location,
            "trigger_interval": self.config.trigger_interval,
            "max_offsets_per_trigger": self.config.max_offsets_per_trigger,
            "enable_debug": self.config.enable_debug
        }
        
        config_json = json.dumps(spark_config)
        
        # –ö–æ–º–∞–Ω–¥–∞ spark-submit –¥–ª—è Local —Ä–µ–∂–∏–º–∞
        cmd = [
            "spark-submit",
            "--master", "local[2]",  # üöÄ –ö–õ–Æ–ß–ï–í–û–ï: Local —Ä–µ–∂–∏–º —Å 2 cores
            "--name", self.config.spark_app_name,
            
            # üöÄ LOCAL MODE: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
            "--conf", f"spark.driver.memory={self.config.spark_driver_memory}",
            "--conf", "spark.sql.adaptive.enabled=true",
            "--conf", "spark.sql.adaptive.coalescePartitions.enabled=true",
            "--conf", "spark.serializer=org.apache.spark.serializer.KryoSerializer",
            "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4",
            app_file,
            config_json
        ]
        
        return cmd
    
    def start_streaming(self) -> str:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤ Local —Ä–µ–∂–∏–º–µ"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (Local Mode)...")
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—É
            cmd = self.create_spark_submit_command()
            logger.info(f"üìã –ö–û–ú–ê–ù–î–ê LOCAL MODE: {' '.join(cmd)}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
            self.spark_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            logger.info(f"‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å (Local Mode) –∑–∞–ø—É—â–µ–Ω: PID {self.spark_process.pid}")
            
            # –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            output_lines = []
            start_time = time.time()
            timeout = 120  # 2 –º–∏–Ω—É—Ç—ã –Ω–∞ –∑–∞–ø—É—Å–∫
            
            while True:
                line = self.spark_process.stdout.readline()
                if line:
                    line = line.strip()
                    output_lines.append(line)
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                    if any(keyword in line.upper() for keyword in ['ERROR', 'EXCEPTION', 'FAILED', 'CRITICAL']):
                        logger.error(f"‚ùå Spark ERROR: {line}")
                    elif any(keyword in line for keyword in ['MAIN FUNCTION', '–®–ê–ì', '===', 'Started', 'SUCCESS']):
                        logger.info(f"üéØ Spark APP: {line}")
                    elif 'WARN' in line.upper():
                        logger.warning(f"‚ö†Ô∏è Spark WARN: {line}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—É—Å–∫
                    if "‚úÖ Streaming –∑–∞–ø—É—â–µ–Ω!" in line:
                        logger.info("üéâ Spark Streaming (Local Mode) —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω!")
                        return f"spark_local_{self.spark_process.pid}"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞
                elif self.spark_process.poll() is not None:
                    return_code = self.spark_process.returncode
                    if return_code == 0:
                        logger.info("‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ")
                    else:
                        logger.error(f"‚ùå Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º: {return_code}")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–π–º–∞—É—Ç
                if time.time() - start_time > timeout:
                    logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –∑–∞–ø—É—Å–∫–∞ Spark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
                    break
            
            return f"spark_local_{self.spark_process.pid}"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Spark Streaming: {e}")
            raise
    
    def stop_streaming(self, app_id: str):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"""
        try:
            if self.spark_process and self.spark_process.poll() is None:
                logger.info(f"üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark –ø—Ä–æ—Ü–µ—Å—Å: {self.spark_process.pid}")
                self.spark_process.terminate()
                
                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                try:
                    self.spark_process.wait(timeout=30)
                    logger.info("‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                except subprocess.TimeoutExpired:
                    logger.warning("‚ö†Ô∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ Spark –ø—Ä–æ—Ü–µ—Å—Å–∞")
                    self.spark_process.kill()
                    self.spark_process.wait()
            else:
                logger.info("‚ÑπÔ∏è Spark –ø—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Spark –ø—Ä–æ—Ü–µ—Å—Å–∞: {e}")


# =============================================================================
# AIRFLOW TASKS
# =============================================================================

def start_spark_streaming(**context):
    """–ó–∞–ø—É—Å–∫ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark Streaming Job (Local Mode)...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = SparkStreamConfig()
        
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
        manager = SparkStreamingManagerLocal(config)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º streaming
        app_id = manager.start_streaming()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ ID –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö (—É–±–∏—Ä–∞–µ–º manager –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏)
        context['task_instance'].xcom_push(key='spark_app_id', value=app_id)
        # –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º manager –≤ XCom –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
        # context['task_instance'].xcom_push(key='spark_manager', value=manager)
        
        logger.info(f"‚úÖ Spark Streaming (Local Mode) –∑–∞–ø—É—â–µ–Ω: {app_id}")
        return app_id
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Spark Streaming: {e}")
        raise

def monitor_streaming(**context):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark Streaming (Local Mode)...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º app_id –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
        app_id = context['task_instance'].xcom_pull(key='spark_app_id', task_ids='start_spark_streaming')
        
        if not app_id:
            logger.warning("‚ö†Ô∏è ID Spark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ XCom")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–ª—É—á–µ–Ω–∏—è app_id
            app_id = context['task_instance'].xcom_pull(task_ids='start_spark_streaming')
            
        if app_id:
            logger.info(f"üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {app_id}")
            
            if "spark_local_" in str(app_id):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º PID –∏–∑ app_id
                pid = int(str(app_id).replace("spark_local_", ""))
                logger.info(f"üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark –ø—Ä–æ—Ü–µ—Å—Å–∞ PID: {pid}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                try:
                    import os
                    os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                    logger.info("‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç–∞–µ—Ç")
                except ProcessLookupError:
                    logger.warning("‚ö†Ô∏è Spark –ø—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å: {e}")
        else:
            logger.warning("‚ö†Ô∏è App ID –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞")
        
        # –ñ–¥–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        monitoring_time = 30  # 30 —Å–µ–∫—É–Ω–¥ (—É–º–µ–Ω—å—à–∏–ª–∏ –≤—Ä–µ–º—è)
        logger.info(f"‚è∞ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ç–µ—á–µ–Ω–∏–µ {monitoring_time} —Å–µ–∫—É–Ω–¥...")
        time.sleep(monitoring_time)
        
        logger.info("‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return "monitoring_completed"
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        return "monitoring_failed"

def stop_streaming(**context):
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming (Local Mode)...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º app_id –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á
        app_id = context['task_instance'].xcom_pull(key='spark_app_id')
        
        if app_id and "spark_local_" in app_id:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º PID –∏–∑ app_id
            pid = int(app_id.replace("spark_local_", ""))
            logger.info(f"üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark –ø—Ä–æ—Ü–µ—Å—Å PID: {pid}")
            
            try:
                import os
                import signal
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º SIGTERM –¥–ª—è graceful shutdown
                os.kill(pid, signal.SIGTERM)
                logger.info("‚úÖ SIGTERM –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω Spark –ø—Ä–æ—Ü–µ—Å—Å—É")
                
                # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ, –∑–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º
                import time
                time.sleep(5)
                
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                    os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                    logger.warning("‚ö†Ô∏è –ü—Ä–æ—Ü–µ—Å—Å –≤—Å–µ –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞")
                    os.kill(pid, signal.SIGKILL)
                except ProcessLookupError:
                    logger.info("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω")
                    
            except ProcessLookupError:
                logger.info("‚ÑπÔ∏è –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞: {e}")
        else:
            logger.warning("‚ö†Ô∏è App ID –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")
        
        logger.info("‚úÖ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        return "streaming_stopped"
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {e}")
        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        return "stop_failed"

def verify_kafka_results(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka —Ç–æ–ø–∏–∫–µ"""
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka —Ç–æ–ø–∏–∫–µ fraud-predictions...")
    
    try:
        from kafka import KafkaConsumer
        import json
        
        config = SparkStreamConfig()
        
        # –°–æ–∑–¥–∞–µ–º consumer –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        consumer = KafkaConsumer(
            config.kafka_output_topic,
            bootstrap_servers=config.kafka_bootstrap_servers.split(','),
            auto_offset_reset='latest',
            consumer_timeout_ms=10000,  # 10 —Å–µ–∫—É–Ω–¥
            value_deserializer=lambda x: json.loads(x.decode('utf-8')) if x else None
        )
        
        messages_count = 0
        for message in consumer:
            if message.value:
                messages_count += 1
                logger.info(f"üì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ {messages_count}: {message.value}")
                
                if messages_count >= 5:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
                    break
        
        consumer.close()
        
        if messages_count > 0:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {messages_count} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–æ–ø–∏–∫–µ {config.kafka_output_topic}")
        else:
            logger.warning(f"‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–æ–ø–∏–∫–µ {config.kafka_output_topic}")
        
        return f"verified_{messages_count}_messages"
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return "verification_failed"


# =============================================================================
# DAG –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï
# =============================================================================

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è DAG
default_args = {
    'owner': 'mlops',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'tasks8_etap4_spark_streaming_local',
    default_args=default_args,
    description='–®–ê–ì 4: Spark Streaming Job –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (LOCAL MODE - –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú –° –ö–õ–ê–°–¢–ï–†–û–ú)',
    schedule=None,  # –¢–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
    catchup=False,
    max_active_runs=1,
    tags=['spark', 'streaming', 'mlflow', 'kafka', 'fraud-detection', 'local-mode']
)

# –ó–∞–¥–∞—á–∞ 1: –ó–∞–ø—É—Å–∫ Spark Streaming
start_task = PythonOperator(
    task_id='start_spark_streaming',
    python_callable=start_spark_streaming,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
monitor_task = PythonOperator(
    task_id='monitor_streaming',
    python_callable=monitor_streaming,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 3: –û—Å—Ç–∞–Ω–æ–≤–∫–∞
stop_task = PythonOperator(
    task_id='stop_streaming',
    python_callable=stop_streaming,
    dag=dag,
    trigger_rule='all_done'  # –í—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —É—Å–ø–µ—Ö–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á
)

# –ó–∞–¥–∞—á–∞ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
verify_task = PythonOperator(
    task_id='verify_results',
    python_callable=verify_kafka_results,
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
start_task >> monitor_task >> stop_task
start_task >> verify_task >> stop_task