"""
–®–ê–ì 4: Spark Streaming Job –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

–≠—Ç–æ—Ç DAG —É–ø—Ä–∞–≤–ª—è–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –∫–æ—Ç–æ—Ä–æ–µ:
- –ß–∏—Ç–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ transactions-input
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç MLflow –º–æ–¥–µ–ª—å fraud_detection_model —Å stage Production
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
- –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Kafka —Ç–æ–ø–∏–∫ fraud-predictions
- –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —á–µ—Ä–µ–∑ checkpoints

–ê–≤—Ç–æ—Ä: MLOps Task 8
"""

import logging
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import subprocess
import signal
import psutil

# Airflow imports
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.context import Context

# Spark –∏ ML imports (–±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark executors)
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    import mlflow
    import mlflow.spark
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    logging.warning("‚ö†Ô∏è Spark/MLflow –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Airflow - –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Spark cluster")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø SPARK STREAMING
# =============================================================================

class SparkStreamConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    def __init__(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ Airflow Variables"""
        try:
            # Kafka –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.kafka_bootstrap_servers = Variable.get("KAFKA_BOOTSTRAP_SERVERS")
            self.kafka_input_topic = Variable.get("KAFKA_INPUT_TOPIC")
            self.kafka_output_topic = Variable.get("KAFKA_OUTPUT_TOPIC")
            self.kafka_consumer_group = Variable.get("KAFKA_CONSUMER_GROUP")
            
            # MLflow –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.mlflow_tracking_uri = Variable.get("MLFLOW_TRACKING_URI")
            self.mlflow_model_name = Variable.get("MLFLOW_MODEL_NAME")
            self.mlflow_model_stage = Variable.get("MLFLOW_MODEL_STAGE", "Production")
            self.mlflow_tracking_username = Variable.get("MLFLOW_TRACKING_USERNAME", "admin")
            self.mlflow_tracking_password = Variable.get("MLFLOW_TRACKING_PASSWORD", "password")
            
            # Spark –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.spark_app_name = Variable.get("SPARK_APP_NAME", "StreamingFraudInference")
            self.spark_master = Variable.get("SPARK_MASTER", "k8s://https://kubernetes.default.svc:443")
            self.spark_namespace = Variable.get("SPARK_NAMESPACE", "spark")
            self.spark_driver_memory = Variable.get("SPARK_DRIVER_MEMORY", "1g")
            self.spark_executor_memory = Variable.get("SPARK_EXECUTOR_MEMORY", "1g") 
            self.spark_executor_cores = Variable.get("SPARK_EXECUTOR_CORES", "1")
            self.spark_max_executors = Variable.get("SPARK_MAX_EXECUTORS", "2")
            
            # S3/MinIO –¥–ª—è checkpoints
            self.s3_endpoint_url = Variable.get("S3_ENDPOINT_URL")
            self.s3_access_key = Variable.get("S3_ACCESS_KEY")
            self.s3_secret_key = Variable.get("S3_SECRET_KEY")
            self.s3_bucket_name = Variable.get("S3_BUCKET_NAME")
            
            # Streaming –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.trigger_interval = "2 seconds"
            self.checkpoint_location = f"s3a://{self.s3_bucket_name}/spark-checkpoints/fraud-streaming"
            self.max_offsets_per_trigger = 1000
            self.enable_debug = Variable.get("DEBUG_MODE", "false").lower() == "true"
            
            logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark Streaming –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise


# =============================================================================
# SPARK STREAMING APPLICATION CODE
# =============================================================================

SPARK_APP_CODE = '''
import logging
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import mlflow
import mlflow.spark
import sys
import os
import traceback

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    stream=sys.stdout  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤ stdout
)
logger = logging.getLogger(__name__)

print("\\n=== üöÄ SPARK FRAUD DETECTION APPLICATION START ===")
logger.info("üöÄ –ó–∞–ø—É—Å–∫ Spark Fraud Detection –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
print(f"Python version: {sys.version}")
print(f"Arguments: {sys.argv}")

def create_spark_session(config):
    """–°–æ–∑–¥–∞–µ—Ç Spark Session —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    try:
        logger.info("üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ Spark Session...")
        logger.info(f"üìù App name: {config['app_name']}")
        logger.info(f"üìù Checkpoint: {config['checkpoint_location']}")
        logger.info(f"üìù S3 endpoint: {config['s3_endpoint_url']}")
        
        spark = SparkSession.builder \\
            .appName(config["app_name"]) \\
            .config("spark.sql.streaming.checkpointLocation", config["checkpoint_location"]) \\
            .config("spark.sql.streaming.stateStore.maintenanceInterval", "60s") \\
            .config("spark.sql.adaptive.enabled", "true") \\
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\
            .config("spark.hadoop.fs.s3a.endpoint", config["s3_endpoint_url"]) \\
            .config("spark.hadoop.fs.s3a.access.key", config["s3_access_key"]) \\
            .config("spark.hadoop.fs.s3a.secret.key", config["s3_secret_key"]) \\
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \\
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \\
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        logger.info(f"‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ: {spark.version}")
        logger.info(f"‚úÖ Spark UI: {spark.sparkContext.uiWebUrl}")
        return spark
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û—à–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è Spark Session: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise

def load_mlflow_model(config):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ MLflow"""
    try:
        logger.info("ü§ñ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É MLflow –º–æ–¥–µ–ª–∏...")
        logger.info(f"üìù MLflow URI: {config['mlflow_tracking_uri']}")
        logger.info(f"üìù –ú–æ–¥–µ–ª—å: {config['mlflow_model_name']}")
        logger.info(f"üìù Stage: {config['mlflow_model_stage']}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º MLflow tracking URI
        mlflow.set_tracking_uri(config["mlflow_tracking_uri"])
        logger.info("‚úÖ MLflow tracking URI —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å credentials
        if 'mlflow_username' in config and 'mlflow_password' in config:
            import os
            os.environ['MLFLOW_TRACKING_USERNAME'] = config['mlflow_username']
            os.environ['MLFLOW_TRACKING_PASSWORD'] = config['mlflow_password']
            logger.info("‚úÖ MLflow –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        
        # üö® –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º S3 credentials –¥–ª—è MLflow (boto3)
        if 's3_access_key' in config and 's3_secret_key' in config:
            import os
            os.environ['AWS_ACCESS_KEY_ID'] = config['s3_access_key']
            os.environ['AWS_SECRET_ACCESS_KEY'] = config['s3_secret_key']
            if 's3_endpoint_url' in config:
                os.environ['AWS_ENDPOINT_URL'] = config['s3_endpoint_url']
            logger.info("‚úÖ S3 credentials –¥–ª—è MLflow –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            logger.info(f"üìã AWS_ACCESS_KEY_ID: {config['s3_access_key'][:4]}***")
            logger.info(f"üìã AWS_ENDPOINT_URL: {config.get('s3_endpoint_url', 'default')}")
        else:
            logger.warning("‚ö†Ô∏è S3 credentials –ù–ï –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏!")
        
        # –ü–æ–ª—É—á–∞–µ–º latest version –º–æ–¥–µ–ª–∏ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º stage
        client = mlflow.tracking.MlflowClient()
        logger.info("‚úÖ MLflow –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
        
        model_version = client.get_latest_versions(
            name=config["mlflow_model_name"],
            stages=[config["mlflow_model_stage"]]
        )
        
        if not model_version:
            raise Exception(f"–ú–æ–¥–µ–ª—å {config['mlflow_model_name']} —Å–æ stage {config['mlflow_model_stage']} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version[0].version}")
        
        model_uri = f"models:/{config['mlflow_model_name']}/{config['mlflow_model_stage']}"
        logger.info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_uri}")
        
        model = mlflow.spark.load_model(model_uri)
        
        logger.info(f"‚úÖ MLflow –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_uri}")
        logger.info(f"üìä –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏: {model_version[0].version}")
        logger.info(f"üìä –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")
        logger.info("‚úÖ –®–ê–ì 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        return model
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û—à–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ MLflow –º–æ–¥–µ–ª–∏: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise

def create_transaction_schema():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ö–µ–º—É –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
    return StructType([
        StructField("transaction_id", StringType(), True),
        StructField("tx_amount", DoubleType(), True),
        StructField("tx_fraud", IntegerType(), True),
        StructField("account_id", StringType(), True),
        StructField("merchant_id", StringType(), True),
        StructField("timestamp", TimestampType(), True),
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        StructField("tx_time_seconds", IntegerType(), True),
        StructField("tx_time_days", IntegerType(), True),
        StructField("merchant_category", StringType(), True),
        StructField("customer_age", IntegerType(), True),
        StructField("customer_city", StringType(), True),
        StructField("customer_state", StringType(), True)
    ])

def process_fraud_prediction(df, model):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∫ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions_df = model.transform(df)
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        result_df = predictions_df.select(
            col("transaction_id"),
            col("tx_amount"),
            col("account_id"),
            col("merchant_id"),
            col("timestamp"),
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏
            col("prediction").alias("fraud_prediction"),
            when(col("prediction") == 1.0, "FRAUD").otherwise("NORMAL").alias("fraud_label"),
            col("probability").alias("fraud_probability"),
            col("kafka_timestamp"),
            col("offset"),
            col("partition")
        )
        
        logger.info("‚úÖ ML –º–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫ –¥–∞–Ω–Ω—ã–º")
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏: {e}")
        raise

def write_to_kafka_sink(df, config):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka"""
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Kafka
        kafka_df = df.select(
            to_json(struct("*")).alias("value"),
            col("transaction_id").alias("key")
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Kafka sink
        query = kafka_df.writeStream \\
            .format("kafka") \\
            .option("kafka.bootstrap.servers", config["kafka_bootstrap_servers"]) \\
            .option("topic", config["kafka_output_topic"]) \\
            .option("checkpointLocation", config["checkpoint_location"]) \\
            .outputMode("append") \\
            .trigger(processingTime=config["trigger_interval"]) \\
            .start()
        
        logger.info(f"‚úÖ Kafka sink –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {config['kafka_output_topic']}")
        return query
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Kafka sink: {e}")
        raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    print("\\n=== üöÄ MAIN FUNCTION START ===")
    logger.info("üöÄ –û–°–ù–û–í–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è main() –∑–∞–ø—É—â–µ–Ω–∞")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        logger.info("üìã –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏...")
        logger.info(f"üìã sys.argv: {sys.argv}")
        
        if len(sys.argv) <= 1:
            raise Exception("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è JSON –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞")
        
        config_json = sys.argv[1]
        logger.info(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è JSON: {config_json[:200]}...")  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
        
        config = json.loads(config_json)
        logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–∞")
        logger.info(f"üìã App name: {config.get('app_name', 'N/A')}")
        logger.info(f"üìã Kafka servers: {config.get('kafka_bootstrap_servers', 'N/A')}")
        logger.info(f"üìã Input topic: {config.get('kafka_input_topic', 'N/A')}")
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        logger.error(f"‚ùå JSON —Å—Ç—Ä–æ–∫–∞: {sys.argv[1] if len(sys.argv) > 1 else 'None'}")
        raise
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        raise
    
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    
    spark = None
    try:
        # –®–ê–ì 1: –°–æ–∑–¥–∞–µ–º Spark Session
        logger.info("üõ†Ô∏è –®–ê–ì 1: –°–æ–∑–¥–∞–Ω–∏–µ Spark Session...")
        spark = create_spark_session(config)
        logger.info("‚úÖ –®–ê–ì 1 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –®–ê–ì 2: –ó–∞–≥—Ä—É–∂–∞–µ–º MLflow –º–æ–¥–µ–ª—å
        logger.info("ü§ñ –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ MLflow –º–æ–¥–µ–ª–∏...")
        model = load_mlflow_model(config)
        logger.info("‚úÖ –®–ê–ì 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –®–ê–ì 3: –°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É
        logger.info("üìã –®–ê–ì 3: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –¥–∞–Ω–Ω—ã—Ö...")
        schema = create_transaction_schema()
        logger.info(f"‚úÖ –®–ê–ì 3: –°—Ö–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(schema.fields)} –ø–æ–ª–µ–π")
        
        # –®–ê–ì 4: –ü–æ–¥–∫–ª—é—á–∞–µ–º Kafka source
        logger.info("üåä –®–ê–ì 4: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Kafka source...")
        logger.info(f"üìã Kafka servers: {config['kafka_bootstrap_servers']}")
        logger.info(f"üìã Input topic: {config['kafka_input_topic']}")
        logger.info(f"üìã Consumer group: {config['kafka_consumer_group']}")
        logger.info(f"üìã Max offsets per trigger: {config.get('max_offsets_per_trigger', 1000)}")
        
        try:
            logger.info("üîß –°–æ–∑–¥–∞–µ–º Kafka DataStreamReader...")
            
            kafka_df = spark \\
                .readStream \\
                .format("kafka") \\
                .option("kafka.bootstrap.servers", config["kafka_bootstrap_servers"]) \\
                .option("subscribe", config["kafka_input_topic"]) \\
                .option("kafka.group.id", config["kafka_consumer_group"]) \\
                .option("startingOffsets", "latest") \\
                .option("maxOffsetsPerTrigger", config.get("max_offsets_per_trigger", 1000)) \\
                .option("failOnDataLoss", "false") \\
                .option("kafka.session.timeout.ms", "30000") \\
                .option("kafka.request.timeout.ms", "40000") \\
                .option("kafka.connection.max.idle.ms", "300000") \\
                .load()
            
            logger.info("‚úÖ DataStreamReader —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—Ö–µ–º—É
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–µ–º—É Kafka –∏—Å—Ç–æ—á–Ω–∏–∫–∞...")
            kafka_schema = kafka_df.schema
            logger.info(f"üìä Kafka schema: {kafka_schema}")
            
            logger.info(f"‚úÖ –®–ê–ì 4: Kafka source –ø–æ–¥–∫–ª—é—á–µ–Ω: {config['kafka_input_topic']}")
            
        except Exception as kafka_error:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Kafka: {kafka_error}")
            logger.error(f"‚ùå Kafka servers: {config['kafka_bootstrap_servers']}")
            logger.error(f"‚ùå Topic: {config['kafka_input_topic']}")
            logger.error(f"‚ùå Consumer group: {config['kafka_consumer_group']}")
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Kafka: {kafka_error}")
        
        # –®–ê–ì 5: –ü–∞—Ä—Å–∏–º JSON
        logger.info("üìã –®–ê–ì 5: –ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka...")
        parsed_df = kafka_df.select(
            from_json(col("value").cast("string"), schema).alias("data"),
            col("timestamp").alias("kafka_timestamp"),
            col("offset"),
            col("partition")
        ).select("data.*", "kafka_timestamp", "offset", "partition")
        logger.info("‚úÖ –®–ê–ì 5: JSON –ø–∞—Ä—Å–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        # –®–ê–ì 6: –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å
        logger.info("ü§ñ –®–ê–ì 6: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏...")
        predictions_df = process_fraud_prediction(parsed_df, model)
        logger.info("‚úÖ –®–ê–ì 6: ML pipeline –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        # –®–ê–ì 7: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Kafka sink
        logger.info("üåä –®–ê–ì 7: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kafka sink...")
        logger.info(f"üìã Output topic: {config['kafka_output_topic']}")
        output_query = write_to_kafka_sink(predictions_df, config)
        logger.info("‚úÖ –®–ê–ì 7: Kafka sink –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        # –®–ê–ì 8: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π debug
        debug_query = None
        if config.get("enable_debug", False):
            logger.info("üîç –®–ê–ì 8: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ debug –∫–æ–Ω—Å–æ–ª–∏...")
            debug_query = predictions_df.writeStream \\
                .outputMode("append") \\
                .format("console") \\
                .option("truncate", "false") \\
                .option("numRows", 10) \\
                .trigger(processingTime=config["trigger_interval"]) \\
                .start()
            logger.info("‚úÖ –®–ê–ì 8: Debug –∫–æ–Ω—Å–æ–ª—å –≤–∫–ª—é—á–µ–Ω–∞")
        else:
            logger.info("üîç –®–ê–ì 8: Debug –æ—Ç–∫–ª—é—á–µ–Ω")
        
        logger.info("‚úÖ –í–°–ï –®–ê–ì–ò –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–´!")
        logger.info("üöÄ Spark Streaming –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info("üìä –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka...")
        print("=== ‚úÖ STREAMING STARTED SUCCESSFULLY ===")
        
        # –®–ê–ì 9: –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è streaming job...")
        output_query.awaitTermination()
        
        if debug_query:
            debug_query.stop()
            
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û—à–ò–ë–ö–ê –≤ main(): {e}")
        logger.error(f"‚ùå –ü–æ–ª–Ω—ã–π traceback: {traceback.format_exc()}")
        print(f"=== ‚ùå CRITICAL ERROR: {e} ===")
        raise
    finally:
        if spark is not None:
            logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark Session...")
            spark.stop()
            logger.info("‚úÖ Spark Session –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("=== üõë MAIN FUNCTION END ===")

if __name__ == "__main__":
    main()
'''


# =============================================================================
# SPARK STREAMING MANAGER
# =============================================================================

class SparkStreamingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º"""
    
    def __init__(self, config: SparkStreamConfig):
        self.config = config
        self.spark_process = None
        self.app_id = None
        
    def create_spark_submit_command(self) -> List[str]:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É spark-submit –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –∫–æ–¥–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        app_file = "/tmp/spark_fraud_streaming.py"
        with open(app_file, "w") as f:
            f.write(SPARK_APP_CODE)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        spark_config = {
            "app_name": self.config.spark_app_name,
            "kafka_bootstrap_servers": self.config.kafka_bootstrap_servers,
            "kafka_input_topic": self.config.kafka_input_topic,
            "kafka_output_topic": self.config.kafka_output_topic,
            "kafka_consumer_group": self.config.kafka_consumer_group,
            "mlflow_tracking_uri": self.config.mlflow_tracking_uri,
            "mlflow_model_name": self.config.mlflow_model_name,
            "mlflow_model_stage": self.config.mlflow_model_stage,
            "mlflow_username": self.config.mlflow_tracking_username,
            "mlflow_password": self.config.mlflow_tracking_password,
            "s3_endpoint_url": self.config.s3_endpoint_url,
            "s3_access_key": self.config.s3_access_key,
            "s3_secret_key": self.config.s3_secret_key,
            "checkpoint_location": self.config.checkpoint_location,
            "trigger_interval": self.config.trigger_interval,
            "max_offsets_per_trigger": self.config.max_offsets_per_trigger,
            "enable_debug": self.config.enable_debug
        }
        
        config_json = json.dumps(spark_config)
        
        # –ö–æ–º–∞–Ω–¥–∞ spark-submit
        cmd = [
            "spark-submit",
            "--master", self.config.spark_master,
            "--deploy-mode", "client",
            "--name", self.config.spark_app_name,

            # üöÄ –í–ê–†–ò–ê–ù–¢ A: –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –†–ï–°–£–†–°–´ –¥–ª—è MLflow –º–æ–¥–µ–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            "--conf", "spark.driver.memory=1g",
            "--conf", "spark.executor.memory=1g", 
            "--conf", "spark.executor.cores=2",
            "--conf", "spark.dynamicAllocation.maxExecutors=2",
            "--conf", "spark.dynamicAllocation.minExecutors=1",  # ‚ùó –ö–õ–Æ–ß–ï–í–û–ï: –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –º–∏–Ω–∏–º—É–º 1 executor
            "--conf", "spark.dynamicAllocation.initialExecutors=1",  # ‚ùó –ö–õ–Æ–ß–ï–í–û–ï: –°—Ä–∞–∑—É –∑–∞–ø—É—Å–∫–∞–µ–º 1 executor 
            "--conf", "spark.dynamicAllocation.enabled=true",
            "--conf", "spark.dynamicAllocation.shuffleTracking.enabled=true",
            "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4",
            # –£–±–∏—Ä–∞–µ–º --py-files —Ç–∞–∫ –∫–∞–∫ pyspark.zip –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ Airflow
            # PySpark —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —á–µ—Ä–µ–∑ pip –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ PYTHONPATH
            app_file,
            config_json
        ]
        
        return cmd
    
    def start_streaming(self) -> str:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—É
            cmd = self.create_spark_submit_command()
            logger.info(f"üìã –ü–û–õ–ù–ê–Ø –ö–û–ú–ê–ù–î–ê: {' '.join(cmd)}")
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ª–æ–≥–æ–≤
            import threading
            self.log_file_path = "/tmp/spark_streaming_full_logs.txt"
            logger.info(f"üìù –í—Å–µ –ª–æ–≥–∏ Spark –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.log_file_path}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
            self.spark_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            logger.info(f"‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω: PID {self.spark_process.pid}")
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤
            self.all_logs = []
            self.log_lock = threading.Lock()
            
            def continuous_log_reader():
                """–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ —á–∏—Ç–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –ª–æ–≥–∏ Spark"""
                with open(self.log_file_path, "w") as log_file:
                    while True:
                        line = self.spark_process.stdout.readline()
                        if not line:
                            if self.spark_process.poll() is not None:
                                # –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
                                logger.error(f"‚ùå Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º: {self.spark_process.returncode}")
                                break
                            continue
                        
                        line = line.strip()
                        with self.log_lock:
                            self.all_logs.append(line)
                            log_file.write(line + "\n")
                            log_file.flush()
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                        if any(keyword in line.upper() for keyword in ['ERROR', 'EXCEPTION', 'FAILED', 'CRITICAL', 'TRACEBACK']):
                            logger.error(f"‚ùå Spark ERROR: {line}")
                        elif any(keyword in line for keyword in ['SPARK FRAUD DETECTION', 'MAIN FUNCTION', '–®–ê–ì', '===', 'Started', 'Creating']):
                            logger.info(f"üéØ Spark APP: {line}")
                        elif 'WARN' in line.upper():
                            logger.warning(f"‚ö†Ô∏è Spark WARN: {line}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è —á—Ç–µ–Ω–∏—è –ª–æ–≥–æ–≤
            log_thread = threading.Thread(target=continuous_log_reader)
            log_thread.daemon = True
            log_thread.start()
            
            # –ß–∏—Ç–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.info("üìã –ß–∏—Ç–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ Spark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
            initial_logs = []
            start_time = time.time()
            
            while time.time() - start_time < 90:  # 90 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –±–æ–ª—å—à–µ –ª–æ–≥–æ–≤
                if self.spark_process.poll() is not None:
                    # –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
                    remaining_output = self.spark_process.stdout.read()
                    logger.error(f"‚ùå Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ!")
                    logger.error(f"‚ùå STDOUT/STDERR: {remaining_output}")
                    return "streaming_failed"
                
                # –ß–∏—Ç–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —á—Ç–µ–Ω–∏—è (–Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π —Ä–µ–∂–∏–º)
                    import fcntl
                    import os
                    fd = self.spark_process.stdout.fileno()
                    flags = fcntl.fcntl(fd, fcntl.F_GETFL)
                    fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)
                    
                    try:
                        line = self.spark_process.stdout.readline()
                        if line:
                            line = line.strip()
                            initial_logs.append(line)
                            logger.info(f"üìä Spark: {line}")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
                            if any(error in line.upper() for error in ['EXCEPTION', 'ERROR', 'FAILED TO']):
                                logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ Spark: {line}")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω—ã–π —Å—Ç–∞—Ä—Ç
                            if 'Started streaming query' in line or 'StreamingQuery started' in line:
                                logger.info("‚úÖ Spark Streaming query –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                                return "streaming_started"
                    except (BlockingIOError, IOError):
                        # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á—Ç–µ–Ω–∏—è - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                        pass
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ —á—Ç–µ–Ω–∏—è: {e}")
                    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É readline
                    try:
                        line = self.spark_process.stdout.readline()
                        if line:
                            line = line.strip()
                            logger.info(f"üìä Spark: {line}")
                    except:
                        pass
                        
                time.sleep(0.5)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –ø–æ—Å–ª–µ —Ç–∞–π–º–∞—É—Ç–∞
            if self.spark_process.poll() is None:
                logger.info("‚úÖ Spark –ø—Ä–æ—Ü–µ—Å—Å –≤—Å–µ –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ 45 —Å–µ–∫—É–Ω–¥")
                
                # –í—ã–≤–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏ –∏–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –±—É—Ñ–µ—Ä–∞
                if hasattr(self, 'all_logs') and self.all_logs:
                    num_lines = 300 if len(self.all_logs) > 300 else len(self.all_logs)
                    logger.info(f"üìã –ü–æ—Å–ª–µ–¥–Ω–∏–µ {num_lines} —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤ Spark:")
                    logger.info("=" * 60)
                    for line in self.all_logs[-20:]:
                        logger.info(f"  {line}")
                    logger.info("=" * 60)
                
                return "streaming_started"
            else:
                logger.error(f"‚ùå Spark –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –∫–æ–¥–æ–º: {self.spark_process.returncode}")
                
                # –í—ã–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –ª–æ–≥–∏
                if hasattr(self, 'all_logs') and self.all_logs:
                    logger.error(f"üìã –í–°–ï –õ–û–ì–ò SPARK (–≤—Å–µ–≥–æ {len(self.all_logs)} —Å—Ç—Ä–æ–∫):")
                    logger.error("=" * 60)
                    for line in self.all_logs:
                        logger.error(f"  {line}")
                    logger.error("=" * 60)
                    
                    # –ò—â–µ–º –æ—à–∏–±–∫–∏
                    error_lines = [l for l in self.all_logs if any(e in l.upper() for e in ['ERROR', 'EXCEPTION', 'FAILED', 'TRACEBACK'])]
                    if error_lines:
                        logger.error("‚ùå –ù–ê–ô–î–ï–ù–´ –û–®–ò–ë–ö–ò:")
                        for line in error_lines:
                            logger.error(f"  >>> {line}")
                
                return "streaming_failed"
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Spark Streaming: {e}")
            return "streaming_error"
    
    def monitor_streaming(self, duration_minutes: int = 10) -> Dict:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç—ã Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        try:
            if not self.spark_process or self.spark_process.poll() is not None:
                logger.error("‚ùå Spark –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è")
                
                # –£–°–ò–õ–ï–ù–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –õ–û–ì–û–í
                logger.info("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å –ª–æ–≥–∞–º–∏...")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
                possible_log_paths = [
                    "/tmp/spark_streaming_full_logs.txt",
                    getattr(self, 'log_file_path', None),
                    "/tmp/spark_streaming_logs.txt"
                ]
                
                for log_path in possible_log_paths:
                    if log_path:
                        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª: {log_path}")
                        try:
                            import os
                            if os.path.exists(log_path):
                                file_size = os.path.getsize(log_path)
                                logger.info(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω! –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç")
                                
                                with open(log_path, 'r') as f:
                                    saved_logs = f.readlines()
                                    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(saved_logs)} —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤")
                                    
                                    if saved_logs:
                                        # –í—ã–≤–æ–¥–∏–º –í–°–ï –ª–æ–≥–∏, –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50
                                        logger.info("üìã –í–°–ï –õ–û–ì–ò SPARK:")
                                        logger.info("=" * 80)
                                        for i, line in enumerate(saved_logs, 1):
                                            logger.info(f"{i:4d}: {line.strip()}")
                                        logger.info("=" * 80)
                                        
                                        # –ò—â–µ–º –æ—à–∏–±–∫–∏
                                        error_lines = [l for l in saved_logs if any(e in l.upper() for e in ['ERROR', 'EXCEPTION', 'FAILED', 'TRACEBACK'])]
                                        if error_lines:
                                            logger.error("‚ùå –ù–ê–ô–î–ï–ù–´ –û–®–ò–ë–ö–ò –í –õ–û–ì–ê–•:")
                                            logger.error("=" * 80)
                                            for line in error_lines:
                                                logger.error(f"  >>> {line.strip()}")
                                            logger.error("=" * 80)
                                    else:
                                        logger.warning("‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç–æ–π!")
                                break
                            else:
                                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_path}")
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {log_path}: {e}")
                else:
                    logger.error("‚ùå –ù–ò –û–î–ò–ù –§–ê–ô–õ –° –õ–û–ì–ê–ú–ò –ù–ï –ù–ê–ô–î–ï–ù!")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–æ–≥–∏ –∏–∑ –ø–∞–º—è—Ç–∏
                if hasattr(self, 'all_logs') and self.all_logs:
                    logger.info(f"üìä –ù–∞–π–¥–µ–Ω—ã –ª–æ–≥–∏ –≤ –ø–∞–º—è—Ç–∏: {len(self.all_logs)} —Å—Ç—Ä–æ–∫")
                    logger.info("üìã –õ–û–ì–ò –ò–ó –ü–ê–ú–Ø–¢–ò:")
                    logger.info("=" * 80)
                    for i, line in enumerate(self.all_logs, 1):
                        logger.info(f"{i:4d}: {line}")
                    logger.info("=" * 80)
                else:
                    logger.warning("‚ö†Ô∏è –õ–æ–≥–∏ –≤ –ø–∞–º—è—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                
                return {"status": "not_running"}
            
            logger.info(f"üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark Streaming –≤ —Ç–µ—á–µ–Ω–∏–µ {duration_minutes} –º–∏–Ω—É—Ç...")
            
            start_time = time.time()
            end_time = start_time + (duration_minutes * 60)
            
            logs = []
            
            while time.time() < end_time and self.spark_process.poll() is None:
                # –ß–∏—Ç–∞–µ–º –ª–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
                try:
                    line = self.spark_process.stdout.readline()
                    if line:
                        logs.append(line.strip())
                        logger.info(f"Spark: {line.strip()}")
                        
                        # –ò—â–µ–º –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –≤ –ª–æ–≥–∞—Ö
                        if "Started streaming query" in line:
                            logger.info("‚úÖ Streaming query –∑–∞–ø—É—â–µ–Ω")
                        elif "Processed" in line and "rows" in line:
                            logger.info(f"üìä {line.strip()}")
                        elif "ERROR" in line or "Exception" in line:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Spark: {line.strip()}")
                            
                except Exception as read_error:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–≥–æ–≤: {read_error}")
                
                time.sleep(5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
            if self.spark_process.poll() is None:
                status = "running"
                logger.info("‚úÖ Spark Streaming –≤—Å–µ –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            else:
                status = "completed"
                logger.info("üèÅ Spark Streaming –∑–∞–≤–µ—Ä—à–∏–ª—Å—è")
            
            return {
                "status": status,
                "monitoring_duration": duration_minutes,
                "logs_count": len(logs),
                "last_logs": logs[-10:] if logs else []
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
            return {"status": "monitoring_error", "error": str(e)}
    
    def stop_streaming(self) -> str:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"""
        try:
            if not self.spark_process:
                logger.info("‚ÑπÔ∏è Spark –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return "not_running"
            
            if self.spark_process.poll() is not None:
                logger.info("‚ÑπÔ∏è Spark –ø—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω")
                return "already_stopped"
            
            logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark Streaming...")
            
            # Graceful shutdown
            self.spark_process.terminate()
            
            # –ñ–¥–µ–º graceful –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            try:
                self.spark_process.wait(timeout=30)
                logger.info("‚úÖ Spark Streaming –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω gracefully")
                return "stopped_gracefully"
            except subprocess.TimeoutExpired:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–±–∏–≤–∞–µ–º
                logger.warning("‚ö†Ô∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ Spark –ø—Ä–æ—Ü–µ—Å—Å–∞")
                self.spark_process.kill()
                self.spark_process.wait()
                return "stopped_forcefully"
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Spark Streaming: {e}")
            return "stop_error"


# =============================================================================
# AIRFLOW TASK FUNCTIONS
# =============================================================================

def start_spark_streaming_job(**context):
    """–ó–∞–ø—É—Å–∫ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark Streaming Job...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = SparkStreamConfig()
        manager = SparkStreamingManager(config)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º Spark Streaming
        result = manager.start_streaming()
        
        if result == "streaming_started":
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º manager –≤ XCom –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á
            context['task_instance'].xcom_push(key='spark_manager', value={
                'status': 'started',
                'pid': manager.spark_process.pid if manager.spark_process else None,
                'app_name': config.spark_app_name
            })
            
            logger.info("‚úÖ Spark Streaming Job –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return "streaming_job_started"
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Spark Streaming Job")
            return f"streaming_job_failed_{result}"
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Spark Streaming Job: {e}")
        raise

def monitor_spark_streaming(**context):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç—ã Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üìä –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Spark Streaming...")
    
    try:
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = SparkStreamConfig()
        manager = SparkStreamingManager(config)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—É—â–µ–Ω–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–∑ XCom
        spark_info = context['task_instance'].xcom_pull(
            task_ids='start_spark_streaming',
            key='spark_manager'
        )
        
        if not spark_info or spark_info.get('status') != 'started':
            logger.error("‚ùå Spark Streaming –Ω–µ –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return "streaming_not_available"
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        monitoring_duration = 5  # 5 –º–∏–Ω—É—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        result = manager.monitor_streaming(monitoring_duration)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        context['task_instance'].xcom_push(key='monitoring_results', value=result)
        
        logger.info("=" * 60)
        logger.info("üìä –û–¢–ß–ï–¢ –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê SPARK STREAMING")
        logger.info("=" * 60)
        logger.info(f"üéØ –°—Ç–∞—Ç—É—Å: {result.get('status', 'unknown')}")
        logger.info(f"‚è∞ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result.get('monitoring_duration', 0)} –º–∏–Ω—É—Ç")
        logger.info(f"üìã –õ–æ–≥–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: {result.get('logs_count', 0)}")
        
        if result.get('last_logs'):
            logger.info("üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏:")
            for log_line in result['last_logs']:
                logger.info(f"   {log_line}")
        
        logger.info("=" * 60)
        
        if result.get('status') == 'running':
            logger.info("‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return "monitoring_successful"
        else:
            logger.warning("‚ö†Ô∏è Spark Streaming –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return "streaming_completed_early"
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        raise

def stop_spark_streaming(**context):
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming Job...")
    
    try:
        config = SparkStreamConfig()
        manager = SparkStreamingManager(config)
        
        result = manager.stop_streaming()
        
        logger.info(f"üèÅ –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {result}")
        
        # –û—á–∏—â–∞–µ–º XCom –¥–∞–Ω–Ω—ã–µ
        context['task_instance'].xcom_push(key='spark_manager', value={'status': 'stopped'})
        
        return f"streaming_{result}"
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Spark Streaming: {e}")
        raise


# =============================================================================
# DAG –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï
# =============================================================================

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã DAG
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'tasks8_etap4_spark_streaming_v10',
    default_args=default_args,
    description='–®–ê–ì 4: Spark Streaming Job –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ (–í–ê–†–ò–ê–ù–¢ A: —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã)',
    schedule=None,  # –¢–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'spark', 'streaming', 'fraud-detection', 'tasks8']
)

# =============================================================================
# –ó–ê–î–ê–ß–ò DAG
# =============================================================================

# –ó–∞–¥–∞—á–∞ 1: –ó–∞–ø—É—Å–∫ Spark Streaming
start_spark_streaming = PythonOperator(
    task_id='start_spark_streaming',
    python_callable=start_spark_streaming_job,
    dag=dag,
    doc_md="""
    ## –ó–∞–ø—É—Å–∫ Spark Streaming Job
    
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞:
    - –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Kafka —Ç–æ–ø–∏–∫—É transactions-input
    - –ó–∞–≥—Ä—É–∂–∞–µ—Ç MLflow –º–æ–¥–µ–ª—å fraud_detection_model@Production
    - –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∫ –ø–æ—Ç–æ–∫—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    - –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–æ–ø–∏–∫ fraud-predictions
    
    **–ù–∞—Å—Ç—Ä–æ–π–∫–∏:**
    - Consumer group: fraud-detection-group
    - Trigger interval: 2 seconds
    - Checkpoint: S3/MinIO
    """
)

# –ó–∞–¥–∞—á–∞ 2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark Streaming
monitor_streaming = PythonOperator(
    task_id='monitor_streaming',
    python_callable=monitor_spark_streaming,
    dag=dag,
    doc_md="""
    ## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Spark Streaming
    
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç:
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ Spark
    - –°–±–æ—Ä –ª–æ–≥–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    - –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –í—ã—è–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    """
)

# –ó–∞–¥–∞—á–∞ 3: –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming
stop_streaming = PythonOperator(
    task_id='stop_streaming',
    python_callable=stop_spark_streaming,
    dag=dag,
    doc_md="""
    ## –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming
    
    Graceful –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è:
    - –ü–æ–ø—ã—Ç–∫–∞ graceful shutdown
    - –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    - –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
    """
)

# –ó–∞–¥–∞—á–∞ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka
def verify_kafka_results(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤—ã—Ö–æ–¥–Ω–æ–º —Ç–æ–ø–∏–∫–µ Kafka"""
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka —Ç–æ–ø–∏–∫–µ fraud-predictions...")
    
    try:
        from kafka import KafkaConsumer
        import json
        
        config = SparkStreamConfig()
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Kafka consumer –¥–ª—è —á—Ç–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        consumer = KafkaConsumer(
            config.kafka_output_topic,
            bootstrap_servers=config.kafka_bootstrap_servers,
            group_id='fraud-results-checker',
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            consumer_timeout_ms=30000,  # 30 —Å–µ–∫—É–Ω–¥ timeout
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        
        logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å –∫ —Ç–æ–ø–∏–∫—É {config.kafka_output_topic}")
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        message_count = 0
        fraud_predictions = 0
        normal_predictions = 0
        
        for message in consumer:
            message_count += 1
            prediction_data = message.value
            
            fraud_label = prediction_data.get('fraud_label', 'UNKNOWN')
            if fraud_label == 'FRAUD':
                fraud_predictions += 1
            elif fraud_label == 'NORMAL':
                normal_predictions += 1
            
            if message_count >= 100:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —á—Ç–µ–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö 100 —Å–æ–æ–±—â–µ–Ω–∏–π
                break
        
        consumer.close()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("=" * 60)
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò KAFKA PREDICTIONS")
        logger.info("=" * 60)
        logger.info(f"üì® –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {message_count}")
        logger.info(f"üö® FRAUD –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {fraud_predictions}")
        logger.info(f"‚úÖ NORMAL –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {normal_predictions}")
        
        if message_count > 0:
            fraud_rate = (fraud_predictions / message_count) * 100
            logger.info(f"üìà –î–æ–ª—è FRAUD: {fraud_rate:.1f}%")
        
        logger.info("=" * 60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'total_messages': message_count,
            'fraud_predictions': fraud_predictions,
            'normal_predictions': normal_predictions,
            'fraud_rate': fraud_rate if message_count > 0 else 0
        }
        
        context['task_instance'].xcom_push(key='kafka_results', value=results)
        
        if message_count > 0:
            logger.info("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞–π–¥–µ–Ω—ã –≤ Kafka")
            return "results_found"
        else:
            logger.warning("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return "no_results"
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return "verification_error"

verify_results = PythonOperator(
    task_id='verify_results',
    python_callable=verify_kafka_results,
    dag=dag,
    doc_md="""
    ## –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∑–∞–ø–∏—Å–∞–ª–∏—Å—å –≤ Kafka —Ç–æ–ø–∏–∫ fraud-predictions:
    - –ß–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–ø–∏–∫–∞
    - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (FRAUD vs NORMAL)
    - –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    """
)

# =============================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô
# =============================================================================

# –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á
start_spark_streaming >> monitor_streaming >> stop_streaming >> verify_results

# –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è DAG
dag.doc_md = """
# –®–ê–ì 4: Spark Streaming Job –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

–≠—Ç–æ—Ç DAG —É–ø—Ä–∞–≤–ª—è–µ—Ç Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

1. **Spark Streaming** —á–∏—Ç–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ Kafka —Ç–æ–ø–∏–∫–∞ `transactions-input`
2. **MLflow –º–æ–¥–µ–ª—å** –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å stage `Production` –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
3. **–ü–æ—Ç–æ–∫–æ–≤—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å** –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∫ –∫–∞–∂–¥–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
4. **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã** –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤ Kafka —Ç–æ–ø–∏–∫ `fraud-predictions`

## –ó–∞–¥–∞—á–∏ DAG:

1. **start_spark_streaming** - –ó–∞–ø—É—Å–∫ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
2. **monitor_streaming** - 5-–º–∏–Ω—É—Ç–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç—ã
3. **stop_streaming** - Graceful –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
4. **verify_results** - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤—ã—Ö–æ–¥–Ω–æ–º —Ç–æ–ø–∏–∫–µ

## –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Spark:

- **Master**: Kubernetes cluster
- **Consumer group**: fraud-detection-group  
- **Trigger interval**: 2 seconds
- **Checkpoints**: S3/MinIO –¥–ª—è –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
- **Dynamic allocation**: –≤–∫–ª—é—á–µ–Ω–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

- –õ–æ–≥–∏ Spark –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Kafka
- Graceful shutdown —Å cleanup —Ä–µ—Å—É—Ä—Å–æ–≤

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

–≠—Ç–æ—Ç DAG –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—é—â–µ–≥–æ DAG –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ Producer.
–î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –≤–º–µ—Å—Ç–µ —Å Producer DAG –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º Consumer Lag.
"""

if __name__ == "__main__":
    logger.info("DAG –®–ê–ì 4: Spark Streaming Inference –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")