"""
—à–∞–≥ 3: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ consumer lag –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

—ç—Ç–æ—Ç dag –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ kafka –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π –∏ —Ç–æ–ø–∏–∫–æ–≤:
- –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ consumer lag –¥–ª—è –≥—Ä—É–ø–ø—ã fraud-detection-group
- –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ throughput –≤—Ö–æ–¥–Ω–æ–≥–æ –∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–ø–∏–∫–æ–≤
- –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ tps –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤
- backpressure —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è producer –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

–∞–≤—Ç–æ—Ä: mlops task 8
"""

import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import time

# Airflow imports
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.utils.context import Context

# Kafka Admin API
from kafka.admin import KafkaAdminClient
from kafka import KafkaConsumer, TopicPartition
from kafka.errors import KafkaError
import kafka.errors

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ 
# =============================================================================

class KafkaMonitorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Kafka –º–µ—Ç—Ä–∏–∫"""
    
    def __init__(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ Airflow Variables"""
        logger.info("üöÄ INIT: –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ KafkaMonitorConfig")
        try:
            # Kafka –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            self.kafka_bootstrap_servers = Variable.get("KAFKA_BOOTSTRAP_SERVERS")
            self.kafka_consumer_group = Variable.get("KAFKA_CONSUMER_GROUP")
            self.kafka_input_topic = Variable.get("KAFKA_INPUT_TOPIC")
            self.kafka_output_topic = Variable.get("KAFKA_OUTPUT_TOPIC")
            self.kafka_test_topic = Variable.get("KAFKA_TEST_TOPIC")
            
            # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤
            self.max_lag_threshold = int(Variable.get("PERF_TEST_MAX_LAG_THRESHOLD", "1000"))
            self.tps_tolerance = float(Variable.get("PERF_TEST_TPS_TOLERANCE", "0.9"))
            
            # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            self.lag_check_interval = int(Variable.get("MONITORING_KAFKA_LAG_CHECK_INTERVAL", "30"))
            self.stats_interval = int(Variable.get("MONITORING_STATS_INTERVAL", "10"))
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤
            self.alert_email = Variable.get("MONITORING_ALERT_EMAIL", "admin@company.com")
            
            # Debug —Ä–µ–∂–∏–º
            self.debug_mode = Variable.get("DEBUG_MODE", "False").lower() == "True"
            self.environment = Variable.get("ENVIRONMENT", "development")
            
            logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"üîç DEBUG: KAFKA_BOOTSTRAP_SERVERS = {self.kafka_bootstrap_servers}")
            logger.info(f"üîç DEBUG: KAFKA_CONSUMER_GROUP = {self.kafka_consumer_group}")
            logger.info(f"üîç DEBUG: KAFKA_INPUT_TOPIC = {self.kafka_input_topic}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise


# =============================================================================
# kafka –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–ª–∞—Å—Å
# =============================================================================

class KafkaLagMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Consumer Lag –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Kafka"""
    
    def __init__(self, config: KafkaMonitorConfig):
        self.config = config
        self.admin_client = None
        self.consumer = None
        self.metrics_history: List[Dict] = []
        
    def _init_kafka_connections(self):
        """–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ kafka"""
        try:
            # kafka admin client –¥–ª—è –º–µ—Ç—Ä–∏–∫
            self.admin_client = KafkaAdminClient(
                bootstrap_servers=self.config.kafka_bootstrap_servers,
                client_id='fraud-lag-monitor'
            )
            
            # consumer –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ lag
            self.consumer = KafkaConsumer(
            bootstrap_servers=self.config.kafka_bootstrap_servers,
            group_id=f"{self.config.kafka_consumer_group}-monitor",
            enable_auto_commit=False,
            auto_offset_reset='latest'
        )
            
            logger.info(f"‚úÖ kafka –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {self.config.kafka_bootstrap_servers}")
            
        except Exception as e:
            logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ kafka: {e}")
            raise
    
    def get_consumer_lag_info(self) -> Dict:
        """–ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ consumer lag"""
        try:
            if not self.consumer:
                self._init_kafka_connections()
            
            # –ø–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Ç–æ–ø–∏–∫–∞–º
            topics_to_monitor = [
                self.config.kafka_input_topic,
                self.config.kafka_output_topic,
                self.config.kafka_test_topic
            ]
            
            lag_info = {
                'timestamp': datetime.now().isoformat(),
                'consumer_group': self.config.kafka_consumer_group,
                'topics': {}
            }
            
            for topic in topics_to_monitor:
                try:
                    # –ø–æ–ª—É—á–∞–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–∏ —Ç–æ–ø–∏–∫–∞
                    partitions = self.consumer.partitions_for_topic(topic)
                    if not partitions:
                        logger.warning(f"‚ö†Ô∏è —Ç–æ–ø–∏–∫ {topic} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π")
                        lag_info['topics'][topic] = {
                            'error': 'topic_not_found',
                            'status': 'missing'
                        }
                        continue
                    
                    topic_info = {
                        'partitions': {},
                        'total_lag': 0,
                        'total_current_offset': 0,
                        'total_log_end_offset': 0,
                        'consumer_group_exists': False
                    }
                    
                    # üîß –ò–°–ü–†–ê–í–õ–ï–ù–û v8.1: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å end_offsets()
                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π consumer –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è high water marks
                    end_offsets = {}
                    committed_offsets = {}
                    
                    try:
                        # –°–æ–∑–¥–∞–µ–º —Ç–æ–ø–∏–∫ –ø–∞—Ä—Ç–∏—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Ç–∏—Ü–∏–π
                        topic_partitions = [TopicPartition(topic, p) for p in partitions]
                        
                        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π consumer –±–µ–∑ –≥—Ä—É–ø–ø—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è end offsets
                        temp_consumer = KafkaConsumer(
                            bootstrap_servers=self.config.kafka_bootstrap_servers,
                            auto_offset_reset='latest',
                            enable_auto_commit=False,
                            value_deserializer=lambda x: x,  # –ü—Ä–æ—Å—Ç–æ–π deserializer
                            group_id=None  # –ë–µ–∑ group_id
                        )
                        
                        # –ü–æ–ª—É—á–∞–µ–º end offsets (high water marks) - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–æ–ø–∏–∫–µ
                        logger.debug(f"üîç –ü–æ–ª—É—á–∞–µ–º end_offsets –¥–ª—è {len(topic_partitions)} –ø–∞—Ä—Ç–∏—Ü–∏–π...")
                        end_offsets = temp_consumer.end_offsets(topic_partitions)
                        logger.debug(f"‚úÖ end_offsets –ø–æ–ª—É—á–µ–Ω—ã: {end_offsets}")
                        
                        temp_consumer.close()
                        
                        # –ü–æ–ª—É—á–∞–µ–º committed offsets —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω–æ–π consumer (–µ—Å–ª–∏ group —Å—É—â–µ—Å—Ç–≤—É–µ—Ç) 
                        try:
                            committed_offsets = self.consumer.committed({tp: None for tp in topic_partitions})
                            logger.debug(f"‚úÖ committed_offsets –ø–æ–ª—É—á–µ–Ω—ã: {committed_offsets}")
                        except Exception as e:
                            logger.debug(f"üîç Consumer group '{self.config.kafka_consumer_group}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {e}")
                            committed_offsets = {tp: None for tp in topic_partitions}
                        
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è offsets: {e}")
                        # Fallback to empty
                        end_offsets = {TopicPartition(topic, p): 0 for p in partitions}
                        committed_offsets = {TopicPartition(topic, p): None for p in partitions}
                    
                    for partition in partitions:
                        tp = TopicPartition(topic, partition)
                        
                        try:
                            # –ü–æ–ª—É—á–∞–µ–º high water mark (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–æ–ø–∏–∫–µ)
                            log_end_offset = end_offsets.get(tp, 0)
                            
                            # –ü–æ–ª—É—á–∞–µ–º committed offset (–ø–æ–∑–∏—Ü–∏—è consumer –≥—Ä—É–ø–ø—ã)
                            committed = committed_offsets.get(tp)
                            current_offset = committed if committed is not None else 0
                            consumer_group_exists = committed is not None
                            
                            # –í—ã—á–∏—Å–ª—è–µ–º lag
                            lag = log_end_offset - current_offset if consumer_group_exists else 0
                            
                            topic_info['partitions'][partition] = {
                                'current_offset': current_offset,
                                'log_end_offset': log_end_offset,
                                'lag': lag,
                                'consumer_group_exists': consumer_group_exists
                            }
                            
                            topic_info['total_lag'] += lag
                            topic_info['total_current_offset'] += current_offset
                            topic_info['total_log_end_offset'] += log_end_offset
                            topic_info['consumer_group_exists'] = consumer_group_exists or topic_info['consumer_group_exists']
                            
                            logger.debug(f"‚úÖ Partition {partition}: end_offset={log_end_offset}, committed={current_offset}, lag={lag}")
                            
                        except Exception as partition_error:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ partition {partition}: {partition_error}")
                            topic_info['partitions'][partition] = {
                                'error': str(partition_error),
                                'consumer_group_exists': False,
                                'current_offset': 0,
                                'log_end_offset': 0,
                                'lag': 0
                            }
                    
                    # —Å—Ç–∞—Ç—É—Å —Ç–æ–ø–∏–∫–∞
                    if not topic_info['consumer_group_exists']:
                        topic_info['status'] = 'no_active_consumers'
                        logger.info(f"üìä {topic}: –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö consumers, messages={topic_info['total_log_end_offset']}, partitions={len(partitions)}")
                    else:
                        topic_info['status'] = 'active_consumers'
                        logger.info(f"üìä {topic}: lag={topic_info['total_lag']}, "
                                  f"current={topic_info['total_current_offset']}, "
                                  f"end={topic_info['total_log_end_offset']}")
                    
                    lag_info['topics'][topic] = topic_info
                    
                except Exception as e:
                    logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–ø–∏–∫–∞ {topic}: {e}")
                    lag_info['topics'][topic] = {'error': str(e)}
            
            return lag_info
            
        except Exception as e:
            logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è consumer lag: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    def calculate_throughput_metrics(self, lag_info: dict) -> Dict:
        """–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ throughput"""
        try:
            current_time = datetime.now()
            
            # –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            throughput_metrics = {
                'timestamp': current_time.isoformat(),
                'topics': {},
                'alerts': [],
                'overall_health': 'healthy'
            }
            
            for topic, topic_info in lag_info.get('topics', {}).items():
                if 'error' in topic_info and topic_info.get('status') != 'no_active_consumers':
                    continue
                
                total_lag = topic_info.get('total_lag', 0)
                log_end_offset = topic_info.get('total_log_end_offset', 0)
                consumer_group_exists = topic_info.get('consumer_group_exists', False)
                topic_status = topic_info.get('status', 'unknown')
                
                # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è —Ç–æ–ø–∏–∫–∞
                if topic_status == 'no_active_consumers':
                    # –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö consumer'–æ–≤, –Ω–æ —Ç–æ–ø–∏–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç–∞–¥–∏–∏
                    health_status = 'waiting_consumers'
                    if log_end_offset > 0:
                        # –µ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è, –Ω–æ –Ω–µ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π - –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ
                        health_status = 'pending_consumption'
                        
                elif total_lag > self.config.max_lag_threshold:
                    health_status = 'critical'
                    throughput_metrics['overall_health'] = 'critical'
                    
                    alert = {
                        'type': 'consumer_lag_critical',
                        'topic': topic,
                        'lag': total_lag,
                        'threshold': self.config.max_lag_threshold,
                        'message': f"üö® consumer lag –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–ª—è {topic}: {total_lag} > {self.config.max_lag_threshold}"
                    }
                    throughput_metrics['alerts'].append(alert)
                    
                elif total_lag > (self.config.max_lag_threshold * 0.7):
                    health_status = 'warning'
                    if throughput_metrics['overall_health'] == 'healthy':
                        throughput_metrics['overall_health'] = 'warning'
                else:
                    health_status = 'healthy'
                
                # —Ä–∞—Å—á–µ—Ç tps (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è)
                estimated_tps = 0
                if len(self.metrics_history) > 1:
                    prev_metrics = self.metrics_history[-1]
                    time_diff = (current_time - datetime.fromisoformat(prev_metrics['timestamp'])).total_seconds()
                    
                    if time_diff > 0 and topic in prev_metrics.get('topics', {}):
                        prev_offset = prev_metrics['topics'][topic].get('total_log_end_offset', 0)
                        offset_diff = log_end_offset - prev_offset
                        estimated_tps = offset_diff / time_diff
                
                throughput_metrics['topics'][topic] = {
                    'total_lag': total_lag,
                    'log_end_offset': log_end_offset,
                    'health_status': health_status,
                    'estimated_tps': round(estimated_tps, 2),
                    'partitions_count': len(topic_info.get('partitions', {}))
                }
                
                logger.info(f"üìà {topic}: lag={total_lag}, tps~={estimated_tps:.1f}, status={health_status}")
            
            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π)
            self.metrics_history.append({
                'timestamp': current_time.isoformat(),
                'topics': {k: {'total_log_end_offset': v.get('total_log_end_offset', 0)} 
                          for k, v in lag_info.get('topics', {}).items()}
            })
            
            if len(self.metrics_history) > 100:
                self.metrics_history = self.metrics_history[-100:]
            
            return throughput_metrics
            
        except Exception as e:
            logger.error(f"‚ùå –æ—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ throughput –º–µ—Ç—Ä–∏–∫: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    def generate_performance_report(self) -> Dict:
        """–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            logger.info("üìä –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã...")
            
            # –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ lag
            lag_info = self.get_consumer_lag_info()
            
            # –≤—ã—á–∏—Å–ª—è–µ–º throughput –º–µ—Ç—Ä–∏–∫–∏
            throughput_metrics = self.calculate_throughput_metrics(lag_info)
            
            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
            report = {
                'report_timestamp': datetime.now().isoformat(),
                'monitoring_config': {
                    'consumer_group': self.config.kafka_consumer_group,
                    'max_lag_threshold': self.config.max_lag_threshold,
                    'environment': self.config.environment
                },
                'consumer_lag_info': lag_info,
                'throughput_metrics': throughput_metrics,
                'recommendations': []
            }
            
            # –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
            overall_health = throughput_metrics.get('overall_health', 'unknown')
            
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å consumer groups
            topics_without_consumers = []
            topics_with_messages = []
            
            for topic, topic_info in lag_info.get('topics', {}).items():
                if topic_info.get('status') == 'no_active_consumers':
                    topics_without_consumers.append(topic)
                    if topic_info.get('total_log_end_offset', 0) > 0:
                        topics_with_messages.append((topic, topic_info.get('total_log_end_offset', 0)))
            
            # —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
            if topics_without_consumers:
                report['recommendations'].append({
                    'type': 'consumer_setup',
                    'priority': 'high',
                    'message': f'üéØ –∑–∞–ø—É—Å—Ç–∏—Ç–µ spark streaming consumer –¥–ª—è —Ç–æ–ø–∏–∫–æ–≤: {", ".join(topics_without_consumers)}'
                })
                
            if topics_with_messages:
                messages_info = ", ".join([f"{topic}({count})" for topic, count in topics_with_messages])
                report['recommendations'].append({
                    'type': 'pending_processing',
                    'priority': 'medium',
                    'message': f'‚è≥ –µ—Å—Ç—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {messages_info} - –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π'
                })
            
            if overall_health == 'critical':
                report['recommendations'].append({
                    'type': 'scaling',
                    'priority': 'high',
                    'message': 'üîÑ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ spark executors –∏–ª–∏ –ø–∞—Ä—Ç–∏—Ü–∏–π'
                })
                
            elif overall_health == 'warning':
                report['recommendations'].append({
                    'type': 'monitoring',
                    'priority': 'medium', 
                    'message': 'üëÄ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é'
                })
                
            elif not topics_without_consumers:
                report['recommendations'].append({
                    'type': 'optimization',
                    'priority': 'low',
                    'message': '‚úÖ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ, –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏'
                })
            
            logger.info(f"‚úÖ –æ—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å: {overall_health}")
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    def close_connections(self):
        """–∑–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ kafka"""
        try:
            if self.consumer:
                self.consumer.close()
                logger.info("‚úÖ kafka consumer –∑–∞–∫—Ä—ã—Ç")
                
            if self.admin_client:
                self.admin_client.close()
                logger.info("‚úÖ kafka admin client –∑–∞–∫—Ä—ã—Ç")
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –æ—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π: {e}")


# =============================================================================
# airflow task functions
# =============================================================================

def run_lag_monitoring_check(**context):
    """
    –∑–∞–¥–∞—á–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ consumer lag
    –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤—Å–µ—Ö consumer groups
    """
    logger.info("üéØ TASK START: run_lag_monitoring_check –Ω–∞—á–∏–Ω–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
    logger.info("üîç –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ consumer lag...")
    
    try:
        # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        logger.info("üìã STEP: –°–æ–∑–¥–∞–µ–º KafkaMonitorConfig()")
        config = KafkaMonitorConfig()
        monitor = KafkaLagMonitor(config)
        
        # –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        report = monitor.generate_performance_report()
        
        # –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("=" * 80)
        logger.info("üìä –æ—Ç—á–µ—Ç –æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ consumer lag")
        logger.info("=" * 80)
        
        # –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        overall_health = report.get('throughput_metrics', {}).get('overall_health', 'unknown')
        logger.info(f"üå°Ô∏è  –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã: {overall_health.upper()}")
        
        # –¥–µ—Ç–∞–ª–∏ –ø–æ —Ç–æ–ø–∏–∫–∞–º
        for topic, metrics in report.get('throughput_metrics', {}).get('topics', {}).items():
            lag = metrics.get('total_lag', 0)
            tps = metrics.get('estimated_tps', 0)
            health = metrics.get('health_status', 'unknown')
            partitions = metrics.get('partitions_count', 0)
            
            # –ø–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ lag_info
            lag_details = report.get('consumer_lag_info', {}).get('topics', {}).get(topic, {})
            consumer_group_exists = lag_details.get('consumer_group_exists', False)
            topic_status = lag_details.get('status', 'unknown')
            messages_count = lag_details.get('total_log_end_offset', 0)
            
            logger.info(f"üìà {topic}:")
            
            if topic_status == 'no_active_consumers':
                logger.info(f"   ‚îú‚îÄ‚îÄ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–æ–ø–∏–∫–µ: {messages_count:,}")
                logger.info(f"   ‚îú‚îÄ‚îÄ consumer group: –Ω–µ –∞–∫—Ç–∏–≤–µ–Ω")
                logger.info(f"   ‚îú‚îÄ‚îÄ —Å—Ç–∞—Ç—É—Å: {health} (–æ–∂–∏–¥–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π)")
                logger.info(f"   ‚îî‚îÄ‚îÄ –ø–∞—Ä—Ç–∏—Ü–∏–∏: {partitions}")
            else:
                logger.info(f"   ‚îú‚îÄ‚îÄ lag: {lag:,} —Å–æ–æ–±—â–µ–Ω–∏–π")
                logger.info(f"   ‚îú‚îÄ‚îÄ tps: ~{tps} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π/—Å–µ–∫")
                logger.info(f"   ‚îú‚îÄ‚îÄ —Å—Ç–∞—Ç—É—Å: {health}")
                logger.info(f"   ‚îî‚îÄ‚îÄ –ø–∞—Ä—Ç–∏—Ü–∏–∏: {partitions}")
        
        # –∞–ª–µ—Ä—Ç—ã
        alerts = report.get('throughput_metrics', {}).get('alerts', [])
        if alerts:
            logger.info("üö® –∞–∫—Ç–∏–≤–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã:")
            for alert in alerts:
                logger.info(f"   ‚ö†Ô∏è  {alert.get('message', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–ª–µ—Ä—Ç')}")
        
        # —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = report.get('recommendations', [])
        if recommendations:
            logger.info("üí° —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            for rec in recommendations:
                logger.info(f"   üîß [{rec.get('priority', 'unknown')}] {rec.get('message', '–Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è')}")
        
        logger.info("=" * 80)
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ xcom –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á
        context['task_instance'].xcom_push(key='lag_monitoring_report', value=report)
        
        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
        if overall_health == 'critical':
            logger.error("‚ùå –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é!")
            return "critical_issues_detected"
        elif overall_health == 'warning':
            logger.warning("‚ö†Ô∏è –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
            return "warnings_detected"
        else:
            logger.info("‚úÖ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return "monitoring_successful"
            
    except Exception as e:
        logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        raise
    
    finally:
        try:
            monitor.close_connections()
        except:
            pass


def run_continuous_lag_monitoring(**context):
    """
    –∑–∞–¥–∞—á–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ consumer lag
    –≤—ã–ø–æ–ª–Ω—è–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ç–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
    """
    logger.info("üîÑ –∑–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ consumer lag...")
    
    try:
        # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        config = KafkaMonitorConfig()
        monitor = KafkaLagMonitor(config)
        
        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        monitoring_duration = 300  # 5 –º–∏–Ω—É—Ç
        check_interval = config.lag_check_interval  # –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        start_time = time.time()
        reports = []
        
        logger.info(f"‚è∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {monitoring_duration}—Å —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {check_interval}—Å")
        
        while (time.time() - start_time) < monitoring_duration:
            try:
                # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
                report = monitor.generate_performance_report()
                reports.append(report)
                
                # –∫—Ä–∞—Ç–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                overall_health = report.get('throughput_metrics', {}).get('overall_health', 'unknown')
                total_lag = sum([
                    metrics.get('total_lag', 0) 
                    for metrics in report.get('throughput_metrics', {}).get('topics', {}).values()
                ])
                
                elapsed = int(time.time() - start_time)
                logger.info(f"[{elapsed:03d}s] üìä —Å—Ç–∞—Ç—É—Å: {overall_health}, –æ–±—â–∏–π lag: {total_lag:,}")
                
                # –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã
                alerts = report.get('throughput_metrics', {}).get('alerts', [])
                for alert in alerts:
                    if alert.get('type') == 'consumer_lag_critical':
                        logger.error(f"üö® –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–µ—Ä—Ç: {alert.get('message')}")
                
            except Exception as e:
                logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
            
            # –∂–¥–µ–º –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            time.sleep(check_interval)
        
        # –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        logger.info("üìà –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
        
        if reports:
            # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–¥–æ—Ä–æ–≤—å—é —Å–∏—Å—Ç–µ–º—ã
            health_stats = {}
            for report in reports:
                health = report.get('throughput_metrics', {}).get('overall_health', 'unknown')
                health_stats[health] = health_stats.get(health, 0) + 1
            
            logger.info("üìä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã:")
            for health, count in health_stats.items():
                percentage = (count / len(reports)) * 100
                logger.info(f"   {health}: {count} —Ä–∞–∑ ({percentage:.1f}%)")
            
            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—Ç—á–µ—Ç—ã –≤ xcom
            context['task_instance'].xcom_push(key='continuous_monitoring_reports', value=reports)
            
            # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            critical_count = health_stats.get('critical', 0)
            if critical_count > len(reports) * 0.1:  # –±–æ–ª–µ–µ 10% –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö
                logger.error(f"‚ùå —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π: {critical_count}/{len(reports)}")
                return "frequent_critical_issues"
            else:
                logger.info("‚úÖ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return "continuous_monitoring_successful"
        else:
            logger.warning("‚ö†Ô∏è –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return "no_monitoring_data"
            
    except Exception as e:
        logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        raise
    
    finally:
        try:
            monitor.close_connections()
        except:
            pass


# =============================================================================
# dag –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
# =============================================================================

# –ø–∞—Ä–∞–º–µ—Ç—Ä—ã dag
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2)
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'tasks8_etap3_kafka_lag_monitor_v8_1',
    default_args=default_args,
    description='üîß v8.1: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ consumer lag (–ò–°–ü–†–ê–í–õ–ï–ù–û - –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ end_offsets)',
    schedule=timedelta(minutes=10),  # –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç (–Ω–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'kafka', 'monitoring', 'fraud-detection', 'tasks8']
)

# =============================================================================
# –∑–∞–¥–∞—á–∏ dag
# =============================================================================

# –∑–∞–¥–∞—á–∞ 1: –æ–¥–∏–Ω–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ consumer lag
single_lag_check = PythonOperator(
    task_id='single_lag_check',
    python_callable=run_lag_monitoring_check,
    dag=dag,
    doc_md="""
    ## –æ–¥–∏–Ω–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ consumer lag
    
    –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è consumer lag –¥–ª—è –≤—Å–µ—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Ç–æ–ø–∏–∫–æ–≤:
    - transactions-input (–≤—Ö–æ–¥—è—â–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏)
    - fraud-predictions (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) 
    - transactions-input-test (—Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ)
    
    **–º–µ—Ç—Ä–∏–∫–∏:**
    - consumer lag –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º
    - throughput estimation
    - health status –æ—Ü–µ–Ω–∫–∞
    - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–æ–≤
    """
)

# –∑–∞–¥–∞—á–∞ 2: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (–¥–ª—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤)
continuous_lag_monitor = PythonOperator(
    task_id='continuous_lag_monitor',
    python_callable=run_continuous_lag_monitoring,
    dag=dag,
    doc_md="""
    ## –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ consumer lag
    
    –≤—ã–ø–æ–ª–Ω—è–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º 30 —Å–µ–∫—É–Ω–¥.
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤.
    
    **–∞–Ω–∞–ª–∏–∑:**
    - —Ç—Ä–µ–Ω–¥—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è lag
    - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã  
    - –≤—ã—è–≤–ª–µ–Ω–∏–µ —É–∑–∫–∏—Ö –º–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
)

# –∑–∞–¥–∞—á–∞ 3: –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–ø–∏–∫–æ–≤
def analyze_topic_performance(**context):
    """–∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç–æ–ø–∏–∫–æ–≤"""
    logger.info("üìä –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–ø–∏–∫–æ–≤...")
    
    try:
        # –ø–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç—ã –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á
        single_report = context['task_instance'].xcom_pull(
            task_ids='single_lag_check', 
            key='lag_monitoring_report'
        )
        
        continuous_reports = context['task_instance'].xcom_pull(
            task_ids='continuous_lag_monitor',
            key='continuous_monitoring_reports'
        )
        
        if not single_report and not continuous_reports:
            logger.warning("‚ö†Ô∏è –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return "no_data_for_analysis"
        
        # –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        analysis = {
            'analysis_timestamp': datetime.now().isoformat(),
            'single_check_result': 'available' if single_report else 'missing',
            'continuous_monitoring_result': 'available' if continuous_reports else 'missing',
            'performance_summary': {}
        }
        
        if single_report:
            topics = single_report.get('throughput_metrics', {}).get('topics', {})
            for topic, metrics in topics.items():
                analysis['performance_summary'][topic] = {
                    'lag': metrics.get('total_lag', 0),
                    'health': metrics.get('health_status', 'unknown'),
                    'tps': metrics.get('estimated_tps', 0)
                }
        
        logger.info("=" * 60)
        logger.info("üìà –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        logger.info("=" * 60)
        
        for topic, summary in analysis.get('performance_summary', {}).items():
            logger.info(f"üéØ {topic}:")
            logger.info(f"   ‚îú‚îÄ‚îÄ lag: {summary['lag']:,}")  
            logger.info(f"   ‚îú‚îÄ‚îÄ health: {summary['health']}")
            logger.info(f"   ‚îî‚îÄ‚îÄ tps: ~{summary['tps']}")
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        context['task_instance'].xcom_push(key='performance_analysis', value=analysis)
        
        logger.info("‚úÖ –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return "analysis_completed"
        
    except Exception as e:
        logger.error(f"‚ùå –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
        raise

topic_performance_analysis = PythonOperator(
    task_id='topic_performance_analysis',
    python_callable=analyze_topic_performance,
    dag=dag,
    doc_md="""
    ## –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–ø–∏–∫–æ–≤
    
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–¥–∏–Ω–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç–æ–ø–∏–∫–æ–≤.
    """
)

# =============================================================================
# –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
# =============================================================================

# –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞–¥–∞—á
single_lag_check >> topic_performance_analysis
continuous_lag_monitor >> topic_performance_analysis

# –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è dag
dag.doc_md = """
# —à–∞–≥ 3: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ consumer lag

—ç—Ç–æ—Ç dag —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ kafka consumer'–æ–≤ 
–¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞.

## –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

1. **–æ–¥–∏–Ω–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ consumer lag** - —Ä–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
2. **–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç –¥–ª—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤  
3. **–∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

## –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ —Ç–æ–ø–∏–∫–∏:

- `transactions-input` - –≤—Ö–æ–¥—è—â–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –æ—Ç producer
- `fraud-predictions` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã spark streaming –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- `transactions-input-test` - —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

## –º–µ—Ç—Ä–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

- **consumer lag** –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º –∏ –æ–±—â–∏–π
- **throughput** (tps) –æ—Ü–µ–Ω–∫–∞
- **health status** (healthy/warning/critical)
- **–∞–ª–µ—Ä—Ç—ã** –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

## –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤:

- –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π lag: 1000 —Å–æ–æ–±—â–µ–Ω–∏–π (perf_test_max_lag_threshold)
- –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏: 30 —Å–µ–∫—É–Ω–¥ (monitoring_kafka_lag_check_interval)
- consumer group: fraud-detection-group

## –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:

—ç—Ç–æ—Ç dag –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
–¥–ª—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –º–æ–∂–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å—Å—è –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—é—â–µ–≥–æ dag.
"""

if __name__ == "__main__":
    logger.info("dag —à–∞–≥ 3: kafka lag monitor –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")