"""
DAG: simple_fraud_detection_training
Description: ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ñ PySpark, MLflow Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹
Ð’ÐµÑ€ÑÐ¸Ñ: 3.0 (Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ñ MLflow 2.15.1, Airflow Variables Ð¸ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ champion)

Ð¤Ð£ÐÐšÐ¦Ð˜ÐžÐÐÐ›Ð¬ÐÐžÐ¡Ð¢Ð¬:
âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ RandomForest Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… fraud_transactions_fixed_new.parquet
âœ… ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð² MLflow Model Registry
âœ… Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° stage "Production" Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ Tasks8 (MLflow 2.9+)
âœ… ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ spark_stream_inference.py

Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢:
ðŸ† ÐœÐ¾Ð´ÐµÐ»ÑŒ fraud_detection_model@Production Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ð´Ð»Ñ Tasks8!
"""

import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# Ð’ÑÐµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð±ÐµÑ€ÑƒÑ‚ÑÑ Ð¸Ð· Airflow Variables (airflow_variables.json)
# Ð­Ñ‚Ð¾ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡Ð¸Ð²Ð°ÐµÑ‚ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÐµÐ¹

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ DAG
default_args = {
    'owner': 'mlops',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def train_model_python():
    """
    Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ PySpark
    ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾: MLflow 2.15.1, Airflow Variables, Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº
    """
    import mlflow
    import mlflow.spark
    from mlflow.tracking import MlflowClient
    from pyspark.sql import SparkSession
    from pyspark.ml import Pipeline
    from pyspark.ml.feature import VectorAssembler, StandardScaler
    from pyspark.ml.classification import RandomForestClassifier
    from pyspark.ml.evaluation import BinaryClassificationEvaluator
    
    print("Starting fraud detection model training...")
    print(f"MLflow version: {mlflow.__version__}")  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²ÐµÑ€ÑÐ¸Ð¸ MLflow
    
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð· Airflow Variables
    S3_ENDPOINT_URL = Variable.get("S3_ENDPOINT_URL")
    S3_ACCESS_KEY = Variable.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = Variable.get("S3_SECRET_KEY")
    S3_BUCKET_NAME = Variable.get("S3_BUCKET_NAME")
    CLEAN_DATA_PATH = Variable.get("CLEAN_DATA_PATH")
    MLFLOW_TRACKING_URI = Variable.get("MLFLOW_TRACKING_URI")
    MLFLOW_EXPERIMENT_NAME = Variable.get("MLFLOW_EXPERIMENT_NAME")
    MLFLOW_TRACKING_USERNAME = Variable.get("MLFLOW_TRACKING_USERNAME")
    MLFLOW_TRACKING_PASSWORD = Variable.get("MLFLOW_TRACKING_PASSWORD")
    
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ MLflow Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
    os.environ["MLFLOW_TRACKING_USERNAME"] = MLFLOW_TRACKING_USERNAME
    os.environ["MLFLOW_TRACKING_PASSWORD"] = MLFLOW_TRACKING_PASSWORD
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ AWS Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº S3/MinIO (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ S3_ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ)
    os.environ["AWS_ACCESS_KEY_ID"] = S3_ACCESS_KEY
    os.environ["AWS_SECRET_ACCESS_KEY"] = S3_SECRET_KEY
    os.environ["MLFLOW_S3_ENDPOINT_URL"] = S3_ENDPOINT_URL
    os.environ["AWS_S3_ENDPOINT_URL"] = S3_ENDPOINT_URL
    
    print(f"S3 configuration:")
    print(f"  Endpoint: {S3_ENDPOINT_URL}")
    print(f"  Bucket: {S3_BUCKET_NAME}")
    print(f"  Access Key: {S3_ACCESS_KEY[:4]}***")
    
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° MLflow
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
    
    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Spark ÑÐµÑÑÐ¸Ð¸ Ñ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ð¼Ð¸ JAR Ñ„Ð°Ð¹Ð»Ð°Ð¼Ð¸ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ MinIO
    spark = SparkSession.builder \
        .appName("FraudDetectionModel") \
        .config("spark.jars.packages", 
                "org.apache.hadoop:hadoop-aws:3.3.4,"
                "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT_URL) \
        .config("spark.hadoop.fs.s3a.access.key", S3_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", S3_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", 
                "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .config("spark.hadoop.fs.s3a.threads.keepalivetime", "60000") \
        .config("spark.hadoop.fs.s3a.connection.establish.timeout", "30000") \
        .config("spark.hadoop.fs.s3a.connection.timeout", "200000") \
        .config("spark.hadoop.fs.s3a.connection.request.timeout", "200000") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    print("Spark session created successfully")
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Parquet
    print(f"Loading cleaned data from: {CLEAN_DATA_PATH}")
    
    try:
        df = spark.read.parquet(CLEAN_DATA_PATH)
        print(f"Data loaded successfully. Total records: {df.count()}")
        print("Data schema:")
        df.printSchema()
        
        # ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾Ðº
        print("First 5 rows:")
        df.show(5, truncate=False)
        
        # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
        print("Available columns and their types:")
        for field in df.schema.fields:
            print(f"  {field.name}: {field.dataType}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ð¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°
        potential_fraud_cols = [col for col in df.columns if 'fraud' in col.lower() or 'label' in col.lower() or 'class' in col.lower()]
        if potential_fraud_cols:
            print(f"Found potential fraud-related columns: {potential_fraud_cols}")
            for col in potential_fraud_cols:
                unique_vals = df.select(col).distinct().limit(10).collect()
                print(f"  {col} unique values: {[row[0] for row in unique_vals]}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ð¼, Ñ‡Ñ‚Ð¾ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ñ†ÐµÐ»ÐµÐ²Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ
        if 'tx_fraud' not in df.columns:
            print("Warning: 'tx_fraud' column not found. Available columns:", df.columns)
            # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ tx_fraud, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ð¹ ÑÑ‚Ð¾Ð»Ð±ÐµÑ†
            fraud_columns = [col for col in df.columns if 'fraud' in col.lower()]
            if fraud_columns:
                print(f"Found potential fraud columns: {fraud_columns}")
            else:
                raise ValueError("No fraud-related column found in the dataset")
        
    except Exception as e:
        print(f"Error loading data: {str(e)}")
        raise
    
    # Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²ÑƒÑŽ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸
    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
    print(f"Training set size: {train_df.count()}")
    print(f"Test set size: {test_df.count()}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ð¼ Ð±Ð°Ð»Ð°Ð½Ñ ÐºÐ»Ð°ÑÑÐ¾Ð²
    target_col = 'tx_fraud'
    fraud_count = df.filter(df[target_col] == 1).count()
    normal_count = df.filter(df[target_col] == 0).count()
    total_count = df.count()
    
    print(f"Dataset balance:")
    print(f"  Normal transactions (0): {normal_count} ({normal_count/total_count*100:.2f}%)")
    print(f"  Fraud transactions (1): {fraud_count} ({fraud_count/total_count*100:.2f}%)")
    print(f"  Total: {total_count}")
    
    if fraud_count == 0:
        print("WARNING: No fraud transactions found in original tx_fraud column!")
        
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ tx_fraud_scenario
        scenario_fraud_count = df.filter(df['tx_fraud_scenario'] > 0).count()
        print(f"Checking tx_fraud_scenario > 0: {scenario_fraud_count} cases")
        
        if scenario_fraud_count > 0:
            print("Using tx_fraud_scenario > 0 as fraud indicator")
            from pyspark.sql.functions import when
            df = df.withColumn(
                target_col,
                when(df['tx_fraud_scenario'] > 0, 1).otherwise(0)
            )
        else:
            print("Creating artificial fraud cases based on transaction patterns...")
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ»ÑƒÑ‡Ð°Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²
            from pyspark.sql.functions import rand, when
            
            # Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ: Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ (Ñ‚Ð¾Ð¿ 2%) + ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ðµ (0.5%) = Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾
            # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ 98-Ð¹ Ð¿ÐµÑ€Ñ†ÐµÐ½Ñ‚Ð¸Ð»ÑŒ Ð¿Ð¾ ÑÑƒÐ¼Ð¼Ðµ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
            percentile_98 = df.approxQuantile("tx_amount", [0.98], 0.01)[0]
            print(f"98th percentile of tx_amount: {percentile_98}")
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½ÑƒÑŽ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ
            df_with_rand = df.withColumn("random", rand(seed=42))
            
            # ÐŸÐ¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÐºÐ°Ðº Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ:
            # - Ð¢Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ > 98-Ð³Ð¾ Ð¿ÐµÑ€Ñ†ÐµÐ½Ñ‚Ð¸Ð»Ñ (Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÑƒÐ¼Ð¼Ñ‹)
            # - 0.5% ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
            df = df_with_rand.withColumn(
                target_col,
                when(
                    (df_with_rand["tx_amount"] > percentile_98) | 
                    (df_with_rand["random"] < 0.005), 1
                ).otherwise(0)
            ).drop("random")
        
        # ÐŸÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        fraud_count = df.filter(df[target_col] == 1).count()
        normal_count = df.filter(df[target_col] == 0).count()
        total_count = df.count()
        
        print(f"After fraud detection logic:")
        print(f"  Normal transactions (0): {normal_count} ({normal_count/total_count*100:.2f}%)")
        print(f"  Fraud transactions (1): {fraud_count} ({fraud_count/total_count*100:.2f}%)")
        print(f"  Total: {total_count}")
        
        # ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°ÐµÐ¼ train/test split Ñ Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
        train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
        print(f"Updated training set size: {train_df.count()}")
        print(f"Updated test set size: {test_df.count()}")
    
    if fraud_count == 0:
        raise ValueError("Still no fraud transactions found! Cannot train fraud detection model.")
    
    if fraud_count < 100:
        print(f"WARNING: Very few fraud cases ({fraud_count}). Model quality may be poor.")
    
    # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ tx_fraud ÐºÐ°Ðº Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ)
    feature_cols = [col for col in train_df.columns 
                   if col != target_col and 
                   col != 'transaction_id' and  # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ID
                   col != 'tx_datetime' and     # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ/Ð²Ñ€ÐµÐ¼Ñ
                   train_df.schema[col].dataType.typeName() != 'string']
    print(f"Target column: {target_col}")
    print(f"Feature columns: {feature_cols}")
    
    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð°
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=True)
    classifier = RandomForestClassifier(labelCol=target_col, featuresCol="features", numTrees=10, maxDepth=5)
    
    pipeline = Pipeline(stages=[assembler, scaler, classifier])
    
    # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    run_name = f"training_{datetime.now().strftime('%Y%m%d_%H%M')}"
    print(f"Starting MLflow run: {run_name}")
    
    with mlflow.start_run(run_name=run_name):
        print("Training model...")
        model = pipeline.fit(train_df)
        print("Model training completed")
        
        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
        predictions = model.transform(test_df)
        print("Predictions generated")
        
        # ÐžÑ†ÐµÐ½ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸
        evaluator = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol="prediction")
        auc = evaluator.evaluate(predictions)
        print(f"Model AUC: {auc}")
        
        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð½ÐµÑÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        from pyspark.ml.evaluation import MulticlassClassificationEvaluator
        
        # Accuracy
        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol="prediction", metricName="accuracy")
        accuracy = accuracy_evaluator.evaluate(predictions)
        
        # Precision
        precision_evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol="prediction", metricName="weightedPrecision")
        precision = precision_evaluator.evaluate(predictions)
        
        # Recall
        recall_evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol="prediction", metricName="weightedRecall")
        recall = recall_evaluator.evaluate(predictions)
        
        # F1-score
        f1_evaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol="prediction", metricName="f1")
        f1 = f1_evaluator.evaluate(predictions)
        
        print(f"Model Metrics:")
        print(f"  AUC: {auc:.4f}")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1-Score: {f1:.4f}")
        
        # Confusion Matrix
        predictions.groupBy(target_col, "prediction").count().show()
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        mlflow.log_metric("auc", auc)
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)
        mlflow.log_metric("training_samples", train_df.count())
        mlflow.log_metric("test_samples", test_df.count())
        mlflow.log_metric("feature_count", len(feature_cols))
        mlflow.log_metric("fraud_count", fraud_count)
        mlflow.log_metric("fraud_percentage", fraud_count/total_count*100)
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸
        mlflow.log_param("num_trees", 10)
        mlflow.log_param("max_depth", 5)
        mlflow.log_param("feature_columns", str(feature_cols))
        mlflow.log_param("mlflow_version", mlflow.__version__)
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ stage "champion"
        try:
            # 1. Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ run
            mlflow.spark.log_model(model, "fraud_detection_model")
            print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð² MLflow ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
            
            # 2. Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Model Registry
            model_uri = f"runs:/{mlflow.active_run().info.run_id}/fraud_detection_model"
            print(f"ðŸ“‹ URI Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸: {model_uri}")
            
            try:
                # Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
                model_details = mlflow.register_model(
                    model_uri=model_uri,
                    name="fraud_detection_model"
                )
                print(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð² Model Registry: Ð²ÐµÑ€ÑÐ¸Ñ {model_details.version}")
                
                # 3. Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ stage "champion"
                client = mlflow.tracking.MlflowClient()
                
                # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ Ð²ÑÐµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð² "Archived"
                try:
                    existing_versions = client.get_latest_versions("fraud_detection_model")
                    for version in existing_versions:
                        if version.current_stage == "Production":
                            client.transition_model_version_stage(
                                name="fraud_detection_model",
                                version=version.version,
                                stage="Archived"
                            )
                            print(f"ðŸ”„ Ð’ÐµÑ€ÑÐ¸Ñ {version.version} Ð¿ÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ð° Ð² Archived")
                except Exception as e:
                    print(f"â„¹ï¸ ÐÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… production Ð²ÐµÑ€ÑÐ¸Ð¹ Ð¸Ð»Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
                
                # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ ÐºÐ°Ðº "Production" (ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ MLflow)
                client.transition_model_version_stage(
                    name="fraud_detection_model",
                    version=model_details.version,
                    stage="Production"
                )
                
                print(f"ðŸ† ÐœÐ¾Ð´ÐµÐ»ÑŒ {model_details.version} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð° ÐºÐ°Ðº PRODUCTION!")
                print(f"ðŸŽ¯ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Tasks8 ÑÐ¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: fraud_detection_model@Production")
                
            except Exception as register_error:
                print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {register_error}")
                mlflow.log_param("model_registration_error", str(register_error))
                
        except Exception as model_log_error:
            print(f"âŒ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐÐ¯ ÐžÐ¨Ð˜Ð‘ÐšÐ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸: {model_log_error}")
            mlflow.log_param("model_logging_error", str(model_log_error))
            
            # ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± - ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÐºÐ°Ðº Ñ„Ð°Ð¹Ð»
            try:
                import tempfile
                import shutil
                
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
                with tempfile.TemporaryDirectory() as temp_dir:
                    model_path = f"{temp_dir}/spark_model"
                    model.write().overwrite().save(model_path)
                    
                    # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ðº Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚
                    mlflow.log_artifacts(model_path, "spark_model")
                    print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° ÐºÐ°Ðº Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚")
                    
            except Exception as artifact_error:
                print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÐºÐ°Ðº Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚: {artifact_error}")
                mlflow.log_param("artifact_logging_error", str(artifact_error))
        
        print(f"Training completed successfully! AUC: {auc}")
    
    spark.stop()
    print("Spark session stopped")

# DAG Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ
with DAG(
    dag_id="simple_fraud_detection_training",
    default_args=default_args,
    description="ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ñ PySpark, MLflow Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ ÐºÐ°Ðº champion",
    schedule=timedelta(hours=1),  # Ð—Ð°Ð¿ÑƒÑÐº ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‡Ð°Ñ
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['mlops', 'fraud-detection', 'champion-registration', 'tasks8-ready'],
) as dag:
    
    # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Python Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    train_task = PythonOperator(
        task_id="train_model",
        python_callable=train_model_python,
        dag=dag,
    )
    
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡
    train_task