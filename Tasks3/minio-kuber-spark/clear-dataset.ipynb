{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 0. Стартуем Spark с доступом к MinIO\n",
    "# ───────────────────────────────────────────────\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, DoubleType\n",
    ")\n",
    "import socket\n",
    "\n",
    "POD_IP = socket.gethostbyname(socket.gethostname())\n",
    "print(\"driver.host =\", POD_IP)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .master(\"spark://10.233.51.227:7077\")\n",
    "      .appName(\"MinIO integration\")\n",
    "      .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2\")\n",
    "      .config(\"spark.driver.host\", POD_IP)\n",
    "      # → S3A / MinIO\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\",          \"http://192.168.31.201:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\",        \"admin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\",        \"password\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "      .config(\"spark.hadoop.fs.s3a.impl\",              \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "              \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1. Читаем ВСЕ .txt  (sep=',', header=False) + явная схема\n",
    "# ───────────────────────────────────────────────\n",
    "input_path = \"s3a://otus/*.txt\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\",    IntegerType(), True),\n",
    "    StructField(\"tx_datetime\",       StringType(),  True),\n",
    "    StructField(\"customer_id\",       IntegerType(), True),\n",
    "    StructField(\"terminal_id\",       IntegerType(), True),\n",
    "    StructField(\"tx_amount\",         DoubleType(),  True),\n",
    "    StructField(\"tx_time_seconds\",   IntegerType(), True),\n",
    "    StructField(\"tx_time_days\",      IntegerType(), True),\n",
    "    StructField(\"tx_fraud\",          IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "raw_df = (spark.read\n",
    "           .csv(input_path,\n",
    "                header=False,       # шапка «плохая» – игнорируем\n",
    "                sep=\",\",\n",
    "                schema=schema,\n",
    "                mode=\"DROPMALFORMED\")  # пропускаем совсем битые строки\n",
    ")\n",
    "\n",
    "# Убираем первую строку‑шапку (transaction_id == null)\n",
    "df = raw_df.filter(raw_df.transaction_id.isNotNull())\n",
    "\n",
    "print(\"Схема после чтения:\")\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 2. Drop NA Удаляем строки с пропущенными значениями\n",
    "# ───────────────────────────────────────────────\n",
    "rows_before = df.count()\n",
    "df_clean    = df.dropna()\n",
    "print(f\"DropNA: {rows_before} → {df_clean.count()}\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 3. Удаляем выбросы (3 σ) по всем числовым колонкам\n",
    "# ───────────────────────────────────────────────\n",
    "from pyspark.sql.types import NumericType\n",
    "from functools import reduce\n",
    "\n",
    "numeric_cols = [f.name for f in df_clean.schema.fields\n",
    "                if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "stats_row = (df_clean\n",
    "             .select(*[F.mean(c).alias(f\"{c}_mean\") for c in numeric_cols],\n",
    "                     *[F.stddev(c).alias(f\"{c}_std\") for c in numeric_cols])\n",
    "             .collect()[0])\n",
    "\n",
    "conds = []\n",
    "for c in numeric_cols:\n",
    "    mu = stats_row[f\"{c}_mean\"]; sigma = stats_row[f\"{c}_std\"]\n",
    "    if sigma:  # если σ не None\n",
    "        low, hi = mu - 3*sigma, mu + 3*sigma\n",
    "        conds.append((F.col(c) >= low) & (F.col(c) <= hi))\n",
    "\n",
    "df_no_outliers = df_clean.filter(reduce(lambda a, b: a & b, conds)\n",
    "                                 if conds else F.lit(True))\n",
    "print(f\"После выбросов: {df_no_outliers.count()} строк\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 4. Логические фильтры\n",
    "# ───────────────────────────────────────────────\n",
    "df_valid = df_no_outliers\n",
    "if \"tx_amount\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"tx_amount\") >= 0)\n",
    "if \"tx_time_seconds\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(\n",
    "        (F.col(\"tx_time_seconds\") >= 0) & (F.col(\"tx_time_seconds\") <= 86400)\n",
    "    )\n",
    "if \"customer_id\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"customer_id\").isNotNull())\n",
    "if \"terminal_id\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"terminal_id\").isNotNull())\n",
    "\n",
    "print(f\"Логические фильтры: {df_no_outliers.count()} → {df_valid.count()}\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 5. Сохраняем корректный Parquet\n",
    "# ───────────────────────────────────────────────\n",
    "output_path = \"s3a://otus/clean/fraud_transactions_fixed_new.parquet\"\n",
    "\n",
    "(df_valid\n",
    "   .write\n",
    "   .mode(\"overwrite\")\n",
    "   .parquet(output_path))\n",
    "\n",
    "print(f\"✅ Новый Parquet сохранён: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Старая версия\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "\n",
    "# --- 0. Сессия Spark ------------------------------------------\n",
    "POD_IP = socket.gethostbyname(socket.gethostname())\n",
    "print(\"driver.host =\", POD_IP)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .master(\"spark://10.233.51.227:7077\")\n",
    "      .appName(\"MinIO integration\")\n",
    "      .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.2\")\n",
    "      .config(\"spark.driver.host\", POD_IP)\n",
    "\n",
    "      # доступ к MinIO\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\",  \"http://192.168.31.201:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\",\"admin\") # в будущем в отдельный файл или секрет сервиса\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\",\"password\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "      .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "      # таймауты + провайдер\n",
    "      .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\",        \"60000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"30000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.timeout\",           \"200000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.request.timeout\",   \"200000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.retry.sleep.max\",              \"30000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "              \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "      .config(\"spark.hadoop.fs.s3a.multipart.purge.age\",          \"86400\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# --- 1. Загружаем исходный датасет ------------------------------------------\n",
    "\n",
    "input_path = \"s3a://otus/*.txt\"        # все 40 .txt в бакете otus\n",
    "df = (spark.read\n",
    "       .csv(input_path, header=True, inferSchema=True, sep=\",\")  # поправь sep, если нужно\n",
    ")\n",
    "\n",
    "# # --- 1. Загружаем ТОЛЬКО 5 файлов для теста ---------------------------------\n",
    "# # input_path = \"s3a://otus/*.txt\"        # полный датасет — пока закомментирован\n",
    "# test_files = [\n",
    "#     \"s3a://otus/2019-08-22.txt\",\n",
    "#     # \"s3a://otus/2019-09-21.txt\",\n",
    "#     # \"s3a://otus/2019-10-21.txt\",\n",
    "#     # \"s3a://otus/2019-11-20.txt\",\n",
    "#     # \"s3a://otus/2019-12-20.txt\",\n",
    "# ]\n",
    "# df = (spark.read\n",
    "#       .csv(test_files, header=True, inferSchema=True, sep=\",\")\n",
    "# )\n",
    "\n",
    "print(\"Схема датафрейма:\")\n",
    "df.printSchema()\n",
    "\n",
    "# --- 2. Удаляем строки с пропущенными значениями ----------------------------\n",
    "rows_before = df.count()\n",
    "df_clean = df.dropna()\n",
    "rows_after = df_clean.count()\n",
    "\n",
    "print(f\"Строк до очистки : {rows_before}\")\n",
    "print(f\"Строк после dropna: {rows_after}\")\n",
    "\n",
    "\n",
    "# --- 3. Удаляем выбросы для КАЖДОЙ числовой колонки ------------------------\n",
    "\n",
    "from pyspark.sql.types import NumericType\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# а) собираем список числовых столбцов\n",
    "numeric_cols = [f.name for f in df_clean.schema.fields\n",
    "                if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "# б) собираем μ и σ сразу по всем числовым колонкам\n",
    "stats_row = (df_clean\n",
    "             .select(*[F.mean(c).alias(f\"{c}_mean\") for c in numeric_cols],\n",
    "                     *[F.stddev(c).alias(f\"{c}_std\") for c in numeric_cols])\n",
    "             .collect()[0])\n",
    "\n",
    "# в) строим список условий вида  col BETWEEN (μ-3σ) AND (μ+3σ)\n",
    "conditions = []\n",
    "for c in numeric_cols:\n",
    "    mu = stats_row[f\"{c}_mean\"]\n",
    "    sigma = stats_row[f\"{c}_std\"]\n",
    "    if sigma is None:          # бывает, если все значения одинаковые\n",
    "        continue\n",
    "    low, high = mu - 3 * sigma, mu + 3 * sigma\n",
    "    conditions.append((F.col(c) >= low) & (F.col(c) <= high))\n",
    "\n",
    "# г) применяем AND‑фильтр по всем условиям\n",
    "df_no_outliers = df_clean.filter(reduce(lambda a, b: a & b, conditions)\n",
    "                                 if conditions else F.lit(True))\n",
    "\n",
    "print(f\"Строк после фильтра выбросов: {df_no_outliers.count()}\")\n",
    "\n",
    "# --- 4. Логические фильтры \"неадекватных\" значений -------------------------\n",
    "# Здесь мы оставляем строки, которые выполняют адекватные условия.\n",
    "# Фильтры применяем ТОЛЬКО если колонка реально существует.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "rows_before_logic = df_no_outliers.count()\n",
    "df_valid = df_no_outliers   # начинаем с того, что уже отфильтровали по выбросам\n",
    "\n",
    "# 4.1 Положительная сумма транзакции\n",
    "if \"tx_amount\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"tx_amount\") >= 0)\n",
    "\n",
    "# 4.2 Время в секундах должно лежать в пределах суток\n",
    "if \"tx_time_seconds\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(\n",
    "        (F.col(\"tx_time_seconds\") >= 0) & (F.col(\"tx_time_seconds\") <= 86400)\n",
    "    )\n",
    "\n",
    "# 4.3 Не пустые идентификаторы клиента и терминала\n",
    "if \"customer_id\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"customer_id\").isNotNull())\n",
    "if \"terminal_id\" in df_valid.columns:\n",
    "    df_valid = df_valid.filter(F.col(\"terminal_id\").isNotNull())\n",
    "\n",
    "rows_after_logic = df_valid.count()\n",
    "print(f\"Строк после логических фильтров: {rows_before_logic} → {rows_after_logic}\")\n",
    "\n",
    "# --- 5. Сохраняем очищенный датасет в формате Parquet ---------------------\n",
    "\n",
    "output_path = \"s3a://otus/clean/fraud_transactions_clean.parquet\"\n",
    "\n",
    "(\n",
    "    df_valid\n",
    "      .write\n",
    "      .mode(\"overwrite\")  # перезаписывает целиком; смени на \"append\", если нужно дописывать\n",
    "      .parquet(output_path)\n",
    ")\n",
    "\n",
    "print(f\"✅ Паркет сохранён: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (feast venv)",
   "language": "python",
   "name": "feast-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
